{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-small.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-small.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext (author's copy with deviant page numbering)](https://shebanq.ancient-data.org/static/docs/methods/2014_Dyk_jnsl.pdf)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* process the set of filled in correction sheets\n",
    "* generate sheets with computed, new features (based on corrected values, valence related) to be edited manually\n",
    "* transform the set of filled in enrichment sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "Between the third and the fourth task, the sheets will be inspected and improved by Janet.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "The results of the enrichment will be added as new features in that same annotation space.\n",
    "\n",
    "## 1.1 Limitations\n",
    "We restrict ourselves to verb occurrences where the verb is the nucleus of a phrase with function *predicate*. \n",
    "There are also verb occurrences in other kinds of phrases, and these also can have complements. These cases are coded very differently in the database. See for example [Joshua 3:8](https://shebanq.ancient-data.org/hebrew/text?book=Josua&chapter=3&verse=8&version=4b&mr=m&qw=q&tp=txt_tb1&tr=hb&wget=v&qget=v&nget=v). (*and you command the priest carrying the ark* ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.8.3\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n",
      "20m 42s END\n"
     ]
    }
   ],
   "source": [
    "import sys,os, collections\n",
    "from copy import deepcopy\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: etcbc4b: UP TO DATE\n",
      "  0.01s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.01s DETAIL: COMPILING a: lexicon: UP TO DATE\n",
      "  0.01s USING annox: lexicon DATA COMPILED AT: 2016-07-08T14-32-54\n",
      "  0.02s DETAIL: load main: G.node_anchor_min\n",
      "  0.10s DETAIL: load main: G.node_anchor_max\n",
      "  0.18s DETAIL: load main: G.node_sort\n",
      "  0.23s DETAIL: load main: G.node_sort_inv\n",
      "  0.63s DETAIL: load main: G.edges_from\n",
      "  0.69s DETAIL: load main: G.edges_to\n",
      "  0.75s DETAIL: load main: X. [node]  -> \n",
      "  1.91s DETAIL: load main: X. [node]  <- \n",
      "  2.74s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  3.51s DETAIL: load main: F.etcbc4_db_otype [node] \n",
      "  4.18s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  4.29s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  4.47s DETAIL: load main: F.etcbc4_ft_ls [node] \n",
      "  4.66s DETAIL: load main: F.etcbc4_ft_prs [node] \n",
      "  4.85s DETAIL: load main: F.etcbc4_ft_rela [node] \n",
      "  5.20s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  5.39s DETAIL: load main: F.etcbc4_ft_typ [node] \n",
      "  5.72s DETAIL: load main: F.etcbc4_ft_uvf [node] \n",
      "  5.91s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  6.10s DETAIL: load main: F.etcbc4_sft_chapter [node] \n",
      "  6.11s DETAIL: load main: F.etcbc4_sft_verse [node] \n",
      "  6.12s DETAIL: load main: F.etcbc4_ft_mother [e] \n",
      "  6.19s DETAIL: load main: C.etcbc4_ft_mother -> \n",
      "  6.37s DETAIL: load main: C.etcbc4_ft_mother <- \n",
      "  6.51s DETAIL: load annox lexicon: F.etcbc4_lex_nametype [node] \n",
      "  6.64s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  6.66s INFO: LOADING PREPARED data: please wait ... \n",
      "  6.67s prep prep: G.node_sort\n",
      "  6.72s prep prep: G.node_sort_inv\n",
      "  7.29s prep prep: L.node_up\n",
      "    10s prep prep: L.node_down\n",
      "    16s prep prep: V.verses\n",
      "    16s prep prep: V.books_la\n",
      "    16s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    18s INFO: LOADED PREPARED data\n",
      "    18s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-10-26T14-05-39\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, 'lexicon', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex uvf prs nametype ls\n",
    "        function rela typ\n",
    "        chapter verse\n",
    "    ''','''\n",
    "        mother\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "result_dir = '{}/results'.format(base_dir)\n",
    "all_results = '{}/all.csv'.format(result_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        msg('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    'Pred', 'PreS', 'PreO', 'PreC', 'PtcO',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this phrase is part of another phrase and does not merit its own function value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 23s {'function': {'EPPr', 'PreC', 'NCoS', 'Rela', 'Modi', 'ModS', 'Intj', 'Pred', 'IntS', 'Frnt', 'PreO', 'Nega', 'Ques', 'Adju', 'ExsS', 'Cmpl', 'Supp', 'NCop', 'Subj', 'PrcS', 'Loca', 'Time', 'Voct', 'Conj', 'BoundErr', 'PtcO', 'Objc', 'PreS', 'Exst', 'PrAd'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "inf('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 26s Finding occurrences ...\n",
      " 9m 28s Done\n",
      "Total:     73679 verb occurrences in 70131 clauses\n",
      "Selected:  16209 verb occurrences in 16053 clauses\n",
      " 9m 28s <BR   556 occurrences of which   33 outside a predicate phrase\n",
      " 9m 28s <FH  2629 occurrences of which   59 outside a predicate phrase\n",
      " 9m 28s <LH   890 occurrences of which   10 outside a predicate phrase\n",
      " 9m 28s BR>    54 occurrences of which    3 outside a predicate phrase\n",
      " 9m 28s BW>  2570 occurrences of which   27 outside a predicate phrase\n",
      " 9m 28s CJT    85 occurrences of which    1 outside a predicate phrase\n",
      " 9m 28s CWB  1056 occurrences of which   22 outside a predicate phrase\n",
      " 9m 28s FJM   609 occurrences of which    3 outside a predicate phrase\n",
      " 9m 28s HLK  1554 occurrences of which   30 outside a predicate phrase\n",
      " 9m 28s JRD   377 occurrences of which   16 outside a predicate phrase\n",
      " 9m 28s JY>  1069 occurrences of which   32 outside a predicate phrase\n",
      " 9m 28s NF>   656 occurrences of which   52 outside a predicate phrase\n",
      " 9m 28s NPL   445 occurrences of which   11 outside a predicate phrase\n",
      " 9m 28s NTN  2017 occurrences of which   10 outside a predicate phrase\n",
      " 9m 28s NWS   159 occurrences of which    4 outside a predicate phrase\n",
      " 9m 28s PQD   303 occurrences of which   72 outside a predicate phrase\n",
      " 9m 28s QR>   883 occurrences of which   12 outside a predicate phrase\n",
      " 9m 28s SWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "inf('Finding occurrences ...')\n",
    "occs = collections.defaultdict(list)   # dictionary of all verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list) # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of all verb occurrence nodes per clause node\n",
    "clause_verb_selected = collections.defaultdict(list) # idem but for the occurrences of selected verbs\n",
    "\n",
    "nw = 0\n",
    "nws = 0\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    nw += 1\n",
    "    lex = F.lex.v(w).rstrip('/=[')\n",
    "    pf = F.function.v(L.u('phrase', w))\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u('clause', w)\n",
    "    clause_verb[cn].append(w)\n",
    "    if lex in verbs:\n",
    "        nws += 1\n",
    "        clause_verb_selected[cn].append(w)\n",
    "\n",
    "inf('Done')\n",
    "inf('Total:    {:>6} verb occurrences in {} clauses'.format(nw, len(clause_verb)), withtime=False)\n",
    "inf('Selected: {:>6} verb occurrences in {} clauses'.format(nws, len(clause_verb_selected)), withtime=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    inf('{} {:>5} occurrences of which {:>4} outside a predicate phrase'.format(\n",
    "        verb, \n",
    "        len(occs[verb]),\n",
    "        len(npoccs[verb]),\n",
    "        withtime=False,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 32s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NFa_etcbc4b.csv\n",
      " 9m 33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/FJM_etcbc4b.csv\n",
      " 9m 33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/QRa_etcbc4b.csv\n",
      " 9m 33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BRa_etcbc4b.csv\n",
      " 9m 33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JRD_etcbc4b.csv\n",
      " 9m 33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BWa_etcbc4b.csv\n",
      " 9m 34s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NTN_etcbc4b.csv\n",
      " 9m 34s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JYa_etcbc4b.csv\n",
      " 9m 34s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NPL_etcbc4b.csv\n",
      " 9m 34s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CWB_etcbc4b.csv\n",
      " 9m 34s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oFH_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/HLK_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/PQD_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oBR_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NWS_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CJT_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oLH_etcbc4b.csv\n",
      " 9m 35s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/SWR_etcbc4b.csv\n",
      " 9m 35s 52110  phrases seen 1  time(s)\n",
      " 9m 35s 181    phrases seen 2  time(s)\n",
      " 9m 35s 9      phrases seen 3  time(s)\n",
      " 9m 35s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = T.passage(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = '* ' if wn in npoccs[verb] else ''\n",
    "        row = [cln, wn, passage_label, lnx, np+vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else ''\n",
    "            row.extend((pn, pnp+pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 9m 39s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oBR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 39s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      " 9m 40s <FH: Found   735 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 9m 40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      " 9m 40s BR>: Found   739 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      " 9m 40s BW>: Found   794 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      " 9m 40s CJT: Found   797 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      " 9m 40s CWB: Found   842 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n",
      " 9m 40s FJM: Found   962 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      " 9m 40s HLK: Found  1121 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 9m 40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JRD_etcbc4b.csv\n",
      " 9m 40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JYa_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      " 9m 40s NF>: Found  1225 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 9m 40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      " 9m 40s NTN: Found  1369 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      " 9m 40s NWS: Found  1380 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n",
      " 9m 40s PQD: Found  1406 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      " 9m 40s QR>: Found  1409 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n",
      " 9m 40s SWR: Found  1434 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n",
      " 9m 40s OK: Corrected phrases did not receive multiple corrections\n",
      " 9m 40s OK: all corrected nodes where phrase nodes\n",
      " 9m 40s OK: all corrected values are legal\n",
      " 9m 40s Found 1434 corrections in the phrase function\n",
      " 9m 40s 41205  phrases seen 1  time(s)\n",
      " 9m 40s 94     phrases seen 2  time(s)\n",
      " 9m 40s 3      phrases seen 3  time(s)\n",
      " 9m 40s Total phrases seen: 41302\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            msg('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            inf('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        inf('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                msg('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            inf('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            inf('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            msg('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            inf('OK: all corrected values are legal')\n",
    "    inf('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26m 11s 6 Enrich field specifications OK\n",
      "valence = {adjunct, complement, core}\n",
      "predication = {NA, copula, regular}\n",
      "grammatical = {*, NA, direct_object, indirect_object, principal_direct_object, subject}\n",
      "original = {*, NA, direct_object, indirect_object, principal_direct_object, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, instrument, location, manner, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    indirect_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    indirect_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    msg('Invalid enrich field specification')\n",
    "else:\n",
    "    inf('{} Enrich field specifications OK'.format(nef))\n",
    "for ef in enrich_fields:\n",
    "    inf('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))), withtime=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    p='''Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA''',\n",
    "    c='''Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tObject\tcomplement\tNA\tdirect_object\t\t\t''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28m 22s Enrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "transform = dict(p={}, c={})\n",
    "errors = 0\n",
    "good = 0\n",
    "\n",
    "for kind in ('p', 'c'):\n",
    "    label = 'phrase' if kind == 'p' else 'clause'\n",
    "    for line in enrich_baseline_rules[kind].split('\\n'):\n",
    "        x = line.split('\\t')\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            msg('Wrong number of fields ({} must be {}) in {}'.format(nefields, nef, line))\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values['function']:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, ['']*nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "                msg('Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(f, label, e, val))\n",
    "                errors += 1\n",
    "            else: good += 1\n",
    "if errors:\n",
    "    msg('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    inf('Enrich baseline rules are OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prettyprint the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: '+('{:<15}' * nef)\n",
    "inf(ltpl.format('func', *enrich_fields), withtime=False)\n",
    "for f in sorted(transformp):\n",
    "    sfs = transformp[f]\n",
    "    inf(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*.\n",
    "\n",
    "We will also analyse the direct and indirect objects more precisely and implement heuristics to make a distinction between complements (locative) and indirect objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the direct objects\n",
    "\n",
    "In the target clauses we will find the direct object(s).\n",
    "If there is more than one, we will compute which is the principal one.\n",
    "The others are secundary ones.\n",
    "If there is only one direct object, we do not mark it as principal.\n",
    "\n",
    "An object can be a phrase or a clause.\n",
    "\n",
    "### Clauses as objects\n",
    "We will treat clauses marked as `Objc` by feature `rela` as direct objects.\n",
    "Additionally, we identify clauses marked as `InfC` by feature `typ` as direct objects if they are preceded by the preposition *L* and if there is a direct object phrase elsewhere in the clause.\n",
    "\n",
    "We will not mark all these object clauses as principal direct objects, by rules stated later on.\n",
    "\n",
    "### Implied objects\n",
    "\n",
    "There are many cases where there is a direct object without it being marked as such in the data.\n",
    "Those are cases where there are no objective, unambiguous signals for a direct object.\n",
    "We call them *implied objects*. Examples: \n",
    "\n",
    "* the relativum in relative clauses\n",
    "* complements starting with MN (from) or L (to)\n",
    "\n",
    "In the case of implied objects we have to guess.\n",
    "Initially we assume that there are no implied objects.\n",
    "\n",
    "Later, when we inspect individual cases, we can mark principal objects and implied objects manually\n",
    "for those cases where these rules do not suffice.\n",
    "\n",
    "### Finding the principal direct object\n",
    "\n",
    "When there are multiple direct objects, we use the rules formulated by (Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) to determine which one is the principal one. The rules are stated below where we make some remarks about how we apply them to our data.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "When looking for principal direct objects, we restrict ourselves to direct objects at the phrase level, either being complete phrases, or pronominal suffixes within phrases. The following rules express a preference for the principal direct object. In a given context, we select the direct object that is preferred by applying those rules as the principal direct object. We only apply these rules if there are at least two direct objects.\n",
    "If there is only one direct object, it is not marked as principal.\n",
    "\n",
    "#### Rule 1: pronominal suffixes > preferred above marked objects > unmarked objects\n",
    "\n",
    "In a given clause, we collect all phrases with function ``PreO`` or ``PtcO``. \n",
    "If this collection is non-empty, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise, we proceed as follows.\n",
    "\n",
    "We collect all the phrases with function ``Objc``.\n",
    "If this collection is empty, there will not be a principal object.\n",
    "Otherwise, we split it up in marked and unmarked object phrases.\n",
    "\n",
    "An object phrase is *marked* if and only if it contains, somewhere, the object marker ``>T``.\n",
    "If there are marked object phrases, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 2: determined phrases > undetermined phrases\n",
    "\n",
    "We only arrive here if there are multiple ``Objc`` phrases, neither of which is marked.\n",
    "In this case, we take the textually first one (by rule 3) which has the value ``det`` for its feature ``det``, if there is one, and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 3: earlier phrases > later phrases (by textual order)\n",
    "\n",
    "This rule is implicitly applied if one of the rules before yielded more than one candidate for the principal object. Furthermore, we arrive here if the previous rules have not selected any principal direct object, while we do have more than one ``Objc`` phrase.\n",
    "\n",
    "In this case, we pick the textually first ``Objc`` phrase.\n",
    "\n",
    "### Complements as Objects\n",
    "\n",
    "In some cases, a complement functions as objects, such as in [Genesis 21:13](https://shebanq.ancient-data.org/hebrew/text?nget=v&chapter=21&book=Genesis&qw=n&tp=txt_tb1&version=4b&mr=m) *I make him (into) a people*.\n",
    "\n",
    "Candidates are those complements that: \n",
    "\n",
    "* start with either preposition ``L`` or ``K`` and\n",
    "* the ``L`` or ``K`` in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\n",
    "\n",
    "We are not sure whether these conditions are sufficient to warrant a direct object in all cases.\n",
    "So we generated the preliminary grammatical label ``_promoted_direct_object`` in these cases.\n",
    "\n",
    "In the review phase of the enrichment sheets, these cases must be resolved by changing this label to ``direct_object`` or ``NA``.\n",
    "\n",
    "A promoted object ranks high as a *principal* direct object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 52s Finding direct objects and determining the principal one\n",
      "21m 53s Done\n",
      "21m 53s Clauses with  3 objects                 :    75\n",
      "21m 53s Clauses with  2 objects                 :  2482\n",
      "21m 53s Clauses with  1 objects                 : 27238\n",
      "21m 53s Clauses with  0 objects                 : 40336\n",
      "21m 53s Clauses with  2 complements as objects  :    34\n",
      "21m 53s Clauses with  1 complements as objects  :  3932\n",
      "Clauses with a principal object         :  2548\n",
      "Clauses with a direct object            : 32427\n",
      "Clauses with a cast direct object       :  3966\n",
      "Total number of clauses                 : 70131\n"
     ]
    }
   ],
   "source": [
    "inf('Finding direct objects and determining the principal one')\n",
    "directobjects = set()\n",
    "pdirectobjects = set()\n",
    "clause_objects = collections.defaultdict(set)\n",
    "\n",
    "mobjects = collections.Counter() # count how many clauses have m objects (for each m)\n",
    "cmobjects = collections.Counter() # count how many clauses have m cast objects (for each m)\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d('word', p)\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "\n",
    "for c in clause_verb:\n",
    "    dobjects = {}\n",
    "    dobjects_set = set()\n",
    "    cast = set()\n",
    "    \n",
    "    for p in L.d('phrase', c):\n",
    "        pf = pf_corr.get(p, F.function.v(p))  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            dobjects.setdefault('p_'+pf, set()).add(p)\n",
    "            dobjects_set.add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d('word', p)\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                cast.add(p)\n",
    "                dobjects_set.add(p)\n",
    "    ncast = len(cast)\n",
    "    if ncast:\n",
    "        cmobjects[ncast] += 1\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d('clause', L.u('sentence', c)):\n",
    "        if list(C.mother.v(ac))[0] == c:\n",
    "            cr = F.rela.v(ac)\n",
    "            ct = F.typ.v(ac)\n",
    "            if cr in {'Objc'}: label = cr\n",
    "            elif ct in {'InfC'}: label = ct\n",
    "            else: continue\n",
    "            dobjects.setdefault('c_'+label, set()).add(ac)\n",
    "            dobjects_set.add(ac)\n",
    "            clause_objects[c].add(ac)\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    dobjects_order = sorted(dobjects_set, key=NK)\n",
    "    nobjects = len(dobjects_order)\n",
    "    mobjects[nobjects] += 1\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates = dobjects.get('p_PreO', set()) | dobjects.get('p_PtcO', set()) | cast\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sorted(principal_candidates, key=NK)[0]\n",
    "            break\n",
    "        principal_candidates = dobjects.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sorted(objects_marked, key=NK)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sorted(objects_unmarked, key=NK)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        pdirectobjects.add(principal_object)\n",
    "\n",
    "    if len(dobjects_set):\n",
    "        directobjects |= dobjects_set\n",
    "\n",
    "inf('Done') \n",
    "\n",
    "for (label, n) in sorted(mobjects.items(), key=lambda y: -y[0]):\n",
    "    inf('{:<40}: {:>5}'.format('Clauses with {:>2} objects'.format(label), n))\n",
    "for (label, n) in sorted(cmobjects.items(), key=lambda y: -y[0]):\n",
    "    inf('{:<40}: {:>5}'.format('Clauses with {:>2} complements as objects'.format(label), n))\n",
    "\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a principal object', len(pdirectobjects)), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a direct object', len(directobjects)), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a cast direct object', sum(cmobjects.values())), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Total number of clauses', len(clause_verb)), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "#### Cues for a locative complement\n",
    "\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``# h_loc`` how many H-locales are carried on words in the complement\n",
    "* ``body_part`` is 2 if the phrase starts with the preposition ``L`` followed by a body part, else 0\n",
    "* ``locativity`` ($loc$) a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``#topo``, ``# prep_b``, ``# h_loc`` and ``body_part``.\n",
    "\n",
    "#### Cues for an indirect object\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` or ``>L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` or ``>L`` plus proper name or person reference word occur in the complement\n",
    "* ``indirect object`` ($ind$) a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "\n",
    "#### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "The outcome is $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*)\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$\n",
    "\n",
    "In words:\n",
    "\n",
    "* if there are positive signals for L or I and none for the other, we choose the one for which there are positive signals;\n",
    "* if there are positive signals for both L and I, we follow the majority count, but only if the difference is at least two;\n",
    "* in all other cases we leave it at C: not necessarilty locative and not necessarily indirect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 58s Determinig kind of complements\n",
      "21m 59s Done\n",
      "Phrases of kind C :  17188\n",
      "Phrases of kind L :  12081\n",
      "Phrases of kind I :   7812\n",
      "Total complements :  37081\n",
      "Total phrases     : 214555\n"
     ]
    }
   ],
   "source": [
    "inf('Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d('phrase', c):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d('word', p)\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "inf('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    inf('Phrases of kind {:<2}: {:>6}'.format(label, n), withtime=False)\n",
    "inf('Total complements : {:>6}'.format(ncomplements), withtime=False)\n",
    "inf('Total phrases     : {:>6}'.format(nphrases), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 1 and F.lex.v(words[1] in personal_lexemes)\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    1: '''direct_object => principal_direct_object''',\n",
    "    2: '''non-object => principal_direct_object''',\n",
    "    3: '''non-object => direct_object''',\n",
    "    4: '''direct-object superfluously promoted to direct object''',\n",
    "    5: '''complement => indirect_object''',\n",
    "    6: '''complement => location''',\n",
    "    7: '''predicate complement => indirect_object''',\n",
    "    8: '''predicate complement => location''',\n",
    "}\n",
    "\n",
    "def generic_logic(pn, values):\n",
    "    gl = None\n",
    "    if pn in pdirectobjects:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 1\n",
    "        else:\n",
    "            gl = 2\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'principal_direct_object'\n",
    "    elif pn in directobjects:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv != 'direct_object':\n",
    "            gl = 3\n",
    "            values['original'] = oldv\n",
    "            values['grammatical'] = 'direct_object'\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 5\n",
    "                else:\n",
    "                    values['valence'] = 'adjunct'\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 6\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 7\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 8\n",
    "    return gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases : value, which is true iff that feature has that value for the phrase in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_logic = {\n",
    "    'CJT': [\n",
    "        (\n",
    "            ('semantic', 'benefactive'), \n",
    "            ('function:Adju', has_L, is_lex_personal),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('function:Cmpl', has_H_locale),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('semantic', 'location'),\n",
    "            ('function:Cmpl', is_lex_local),\n",
    "        ),\n",
    "    ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 15s CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      "22m 15s CJT-2 if function   = Cmpl     AND has_H_locale   \n",
      "      lexical    => location       \n",
      "\n",
      "22m 15s CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      "22m 15s All 3 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    result = '{}-{} '.format(vl, i+1)\n",
    "    pref_len = len(result)\n",
    "    result += 'if {}\\n'.format(' AND '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        result += '{}{:<10} => {:<15}\\n'.format(' '* pref_len, *sfa)\n",
    "    return result\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for vl in sorted(enrich_logic):\n",
    "        for items in enrich_logic[vl]:\n",
    "            rule_index[vl].append((items[-1], items[0:-1]))\n",
    "        for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "            inf(rule_as_str(vl, i))\n",
    "            nrules += 1\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                if sf not in enrich_fields:\n",
    "                    msg('\"{}\" not a valid enrich field'.format(sf), withtime=False)\n",
    "                    errors += 1\n",
    "                elif sfval not in enrich_fields[sf]:\n",
    "                    msg('`{}`: \"{}\" not a valid enrich field value'.format(sf, sfval), withtime=False)\n",
    "                    errors += 1\n",
    "            for c in conditions:\n",
    "                if type(c) == str:\n",
    "                    x = c.split(':')\n",
    "                    if len(x) != 2:\n",
    "                        msg('Wrong feature condition {}'.format(c))\n",
    "                        errors += 1\n",
    "                    else:\n",
    "                        (feat, val) = x\n",
    "                        if feat not in legal_values:\n",
    "                            msg('Feature `{}` not in use'.format(feat))\n",
    "                            errors += 1\n",
    "                        elif val not in legal_values[feat]:\n",
    "                            msg('Feature `{}`: not a valid value \"{}\"'.format(feat, val))\n",
    "                            errors += 1\n",
    "    if errors:\n",
    "        msg('There were {} errors in {} rules'.format(errors, nrules))\n",
    "    else:\n",
    "        inf('All {} rules OK'.format(nrules))\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generic_cases = {}\n",
    "applied_cases = {}\n",
    "\n",
    "def apply_logic(vl, pn, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic(pn, values)\n",
    "    if gr:\n",
    "        generic_cases.setdefault(gr, []).append(pn)\n",
    "    verb_rules = enrich_logic.get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                fval = pf_corr.get(pn, F.function.v(pn)) if feature == 'function' else F.item[feature].v(pn)\n",
    "                this_ok =  fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, pn)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            applied_cases.setdefault((vl, i), []).append(pn)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'enrich_fields' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb7d22a34d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mphrase_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m '''.strip().split() + list(enrich_fields)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOMMON_FIELDS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCLAUSE_FIELDS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPHRASE_FIELDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enrich_fields' is not defined"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    type\n",
    "    function\n",
    "'''.strip().split() + list(enrich_fields)\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    type\n",
    "    rela\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = COMMON_FIELDS + PHRASE_FIELDS + CLAUSE_FIELDS\n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows =  pfillrows + cfillrows\n",
    "inf('\\n'.join(field_names), withtime=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 20s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/FJM_etcbc4b.csv (2857 rows)\n",
      "22m 20s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JRD_etcbc4b.csv (1553 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oFH_etcbc4b.csv (11048 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NPL_etcbc4b.csv (1915 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CJT_etcbc4b.csv (375 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oLH_etcbc4b.csv (3821 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NWS_etcbc4b.csv (613 rows)\n",
      "22m 21s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/QRa_etcbc4b.csv (3662 rows)\n",
      "22m 22s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/PQD_etcbc4b.csv (1269 rows)\n",
      "22m 22s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/SWR_etcbc4b.csv (1259 rows)\n",
      "22m 22s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BWa_etcbc4b.csv (10817 rows)\n",
      "22m 22s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BRa_etcbc4b.csv (219 rows)\n",
      "22m 23s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NFa_etcbc4b.csv (2826 rows)\n",
      "22m 23s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/HLK_etcbc4b.csv (5672 rows)\n",
      "22m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NTN_etcbc4b.csv (9659 rows)\n",
      "22m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oBR_etcbc4b.csv (2309 rows)\n",
      "22m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JYa_etcbc4b.csv (4495 rows)\n",
      "22m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CWB_etcbc4b.csv (4238 rows)\n",
      "22m 25s Done\n",
      "3 rules applied\n",
      "direct_object => principal_direct_object\n",
      "\t 527 phrases: 613738, 614118, 621196, 621210, 621811, 622447, 623511, 632122, 644198, 661557\n",
      "\n",
      "non-object => principal_direct_object\n",
      "\t 667 phrases: 608529, 610836, 610922, 616022, 621469, 622077, 627845, 636047, 636261, 668832\n",
      "\n",
      "non-object => direct_object\n",
      "\t 776 phrases: 621197, 621212, 622449, 623512, 661559, 697142, 711390, 722275, 723001, 730776\n",
      "\n",
      "complement => indirect_object\n",
      "\t1638 phrases: 695192, 703965, 703973, 706530, 763769, 768370, 772631, 813099, 617967, 620911\n",
      "\n",
      "complement => location\n",
      "\t4123 phrases: 605598, 606845, 611241, 611635, 611728, 612025, 615144, 615145, 615424, 615587\n",
      "\n",
      "predicate complement => indirect_object\n",
      "\t   4 phrases: 654844, 853847, 691074, 704297\n",
      "\n",
      "predicate complement => location\n",
      "\t  20 phrases: 792494, 792847, 804404, 804575, 847484, 773644, 800764, 816576, 646365, 654810\n",
      "\n",
      "7755 generic applications in total\n",
      "CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      "\t   5 phrases: 615130, 630648, 712015, 794512, 797440\n",
      "\n",
      "CJT-2 if function   = Cmpl     AND has_H_locale   \n",
      "      lexical    => location       \n",
      "\n",
      "\t   1 phrases: 731940\n",
      "\n",
      "CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      "\t   9 phrases: 606396, 619338, 630956, 654145, 731940, 776266, 789542, 794185, 797377\n",
      "\n",
      "15 specific applications in total\n",
      "22m 25s 52110  phrases seen 1  time(s)\n",
      "22m 25s 181    phrases seen 2  time(s)\n",
      "22m 25s 9      phrases seen 3  time(s)\n",
      "22m 25s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "subclauses_seen = collections.Counter()\n",
    "subclauses_objc = collections.Counter()\n",
    "subclauses_infc = collections.Counter()\n",
    "subclauses_good_infc = collections.Counter()\n",
    "\n",
    "\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cl = L.u('clause', wn)\n",
    "        if cl in clauses_seen: continue\n",
    "        clauses_seen.add(cl)\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, '')\n",
    "        rows.append(common_fields + (('',)*fillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            phrases_seen[pn] += 1\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vstem, vt, '', 'phrase')\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields = ('',)*pfillrows + (pt, pty, pf) + apply_logic(vl, pn, transformp[pf])            \n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "        for scn in L.d('clause', cn):\n",
    "            subclauses_seen[scn] += 1\n",
    "            common_fields = (cn, wn, scn, book, chapter, verse, vl, vstem, vt, '', 'clause')\n",
    "            sct = T.words(L.d('word', scn), fmt='ec').replace('\\n', '')\n",
    "            scty = F.typ.v(scn))\n",
    "            scr = F.rela.v(scn))\n",
    "            clause_fields = ('',)*cfillrows + (sct, scty, scr) + apply_logic(vl, scn, transformc[scf])            \n",
    "            rows.append(common_fields + (('',)*cfillrows) + clause_fields)\n",
    "\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated enrichment sheet for verb {} ({} rows)'.format(filename, len(rows)))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "inf('Done')\n",
    "inf('{} rules applied'.format(len(applied_cases)), withtime=False)\n",
    "totaln = 0\n",
    "for rule_id in generic_cases:\n",
    "    cases = generic_cases[rule_id]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    inf('{}\\n\\t{:>4} phrases: {}\\n'.format(\n",
    "        grule_as_str[rule_id], n, ', '.join(str(c) for c in cases[0:10]),\n",
    "    ), withtime=False)\n",
    "inf('{} generic applications in total'.format(totaln), withtime=False)\n",
    "totaln = 0\n",
    "for rule_id in applied_cases:\n",
    "    cases = applied_cases[rule_id]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    inf('{}\\n\\t{:>4} phrases: {}\\n'.format(\n",
    "        rule_as_str(*rule_id), n, ', '.join(str(c) for c in cases[0:10]),\n",
    "    ), withtime=False)\n",
    "inf('{} specific applications in total'.format(totaln), withtime=False)\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showcase(pn):\n",
    "    inf('''{} {}\\n{}'''.format(\n",
    "        pf_corr.get(pn, F.function.v(pn)),\n",
    "        T.words(L.d('word', pn), fmt='ec'), \n",
    "        T.text(\n",
    "            book=F.book.v(L.u('book', pn)), \n",
    "            chapter=int(F.chapter.v(L.u('chapter', pn))),\n",
    "            verse=int(F.verse.v(L.u('verse', pn))), \n",
    "            fmt='ec', lang='la',\n",
    "        ),\n",
    "    ), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreC PQWDJ HLWJ LM#PXTM \n",
      "Numeri 26:57\tW>LH PQWDJ HLWJ LM#PXTM LGR#WN M#PXT HGR#NJ LQHT M#PXT HQHTJ LMRRJ M#PXT HMRRJ00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase(654844)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t26118, 26127, 146447, 187920, 197138, 272406, 95257, 184350, 398368, 289826, 201253, 24616, 78897, 401459, 100410, 32829, 100413, 198208, 5698, 200258, 100938, 24653, 141902, 112207, 186960, 24658, 196690, 28764, 34400, 298594, 248931, 132198, 162918, 12402, 5747, 146044, 396927, 153216, 134792, 151176, 188042, 97419, 426120, 257165, 136338, 21656, 162970, 200349, 214687, 24740, 257192, 158378, 100527, 25777, 160434, 214707, 4789, 4793, 272569, 139963, 90812, 249020, 38595, 113861, 138448, 8920, 282841, 19166, 20703, 26850, 43235, 145127, 8424, 8937, 170729, 397032, 254703, 154354, 200948, 426230, 176376, 79609, 165626, 206075, 208636, 27391, 269569, 106246, 157447, 26380, 149785, 170782, 211232, 126758, 26414, 27438, 246062, 109363, 172340, 249140, 398134, 64828, 26431, 16704, 4929, 168771, 154964, 132955, 393569, 47460, 157541, 47466, 100206, 37232, 269170, 23415, 410999, 23933, 24448, 78208, 133518, 25999, 191381, 12698, 19355, 24476, 170909, 18344, 157608, 267689, 244660, 256952, 8633, 63419, 167359, 175553, 138694, 110536, 175561, 108490, 111051, 143820, 37324, 192973, 264137, 5586, 99795, 11732, 170963, 20438, 218583, 269285, 25062, 110576, 26099, 184315, 256511\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t90243, 93571, 29637, 284965, 289859, 136745, 257293, 289871, 154354, 154964, 9525, 257016, 284989, 93598\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t154354, 322818, 154964, 75702\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u('clause', w)\n",
    "        ps = L.d('phrase', c)\n",
    "        phs = {p for p in ps if len({w for w in L.d('word', p) if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        inf('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), withtime=False)\n",
    "        if show_results: inf('\\t{}'.format(', '.join(str(x) for x in hl[f])), withtime=False)\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oBR_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oFH_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oLH_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BRa_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BWa_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CJT_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CWB_etcbc4b.csv\n",
      "22m 39s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/FJM_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/HLK_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JRD_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JYa_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NFa_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NPL_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NTN_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NWS_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/PQD_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/QRa_etcbc4b.csv\n",
      "22m 40s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 40s OK: The used blank enrichment sheets have legal values\n",
      "22m 40s OK: The used blank enrichment sheets are consistent\n",
      "22m 40s OK: The used filled enrichment sheets have legal values\n",
      "22m 40s OK: The used filled enrichment sheets are consistent\n",
      "22m 40s OK: all enriched nodes where phrase nodes\n",
      "22m 40s OK: all enriched nodes occurred in the blank sheet\n",
      "22m 40s OK: there are 678 manual correction/enrichment annotations\n",
      "COR                                    => 2_Kings 22:6 726393 ,,,,,\n",
      "\tLEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "\t\tW:JIT.:NW. >OTOW L:<OF;J HAM.:L@>K@H LEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "COR                                    => Genesis 1:27 605442 complement,NA,direct_object,,,\n",
      "\tZ@K@R W.N:Q;B@H \n",
      "\t\tZ@K@R W.N:Q;B@H B.@R@> >OT@m00\n",
      "\n",
      "COR                                    => Genesis 5:2 606420 complement,NA,direct_object,,,\n",
      "\tZ@K@R W.N:Q;B@H \n",
      "\t\tZ@K@R W.N:Q;B@H B.:R@>@m \n",
      "COR                                    => Isaiah 4:5 728419 adjunct,NA,NA,,location,location\n",
      "\t<AL K.@L&M:KOWn HAR&YIJ.OWn W:<AL&MIQ:R@>EH@ \n",
      "\t\tW.B@R@> J:HW@H <AL K.@L&M:KOWn HAR&YIJ.OWn W:<AL&MIQ:R@>EH@ <@N@n 05 JOWM@m \n",
      "COR                                    => Psalms 89:48 799849 complement,NA,principal_direct_object,,,\n",
      "\tC.@W:> \n",
      "\t\t<AL&MAH&C.@W:> B.@R@>T@ K@L&B.:N;J&>@D@m00\n",
      "\n",
      "COR                                    => Genesis 45:25 621386 ,,,,,\n",
      "\t>EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "\t\tWAJ.@BO>W. >EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "COR                                    => Exodus 14:16 627778 adjunct,NA,*,,location,location\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tW:J@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H00\n",
      "\n",
      "COR                                    => Exodus 14:22 627852 adjunct,NA,*,,location,location\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tWAJ.@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H \n",
      "COR                                    => Leviticus 4:23 637127 ,,,,,\n",
      "\tF:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "\t\tW:H;BIJ> >ET&Q@R:B.@NOW F:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "COR                                    => Leviticus 4:28 637188 ,,,,,\n",
      "\tF:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H \n",
      "\t\tW:H;BIJ> Q@R:B.@NOW F:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H <AL&XAV.@>TOW \n",
      "... AND 668 ANNOTATIONS MORE\n",
      "22m 41s 52110  phrases seen 1  time(s)\n",
      "22m 41s 181    phrases seen 2  time(s)\n",
      "22m 41s 9      phrases seen 3  time(s)\n",
      "22m 41s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def read_enrich(rootdir): # rootdir will not be used, data is computed from sheets\n",
    "    pf_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_phrase = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                msg('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            #inf('READING {} enrichments file {}'.format(check[1], filename))\n",
    "\n",
    "            with open(filename) as f:\n",
    "                header = f.__next__()\n",
    "                for line in f:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    pn = int(fields[2])\n",
    "                    if pn < 0: continue\n",
    "                    phrases_seen[pn] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][pn].append((verb, f, v))\n",
    "                    vresults[is_filled][pn] = vvals\n",
    "                    if pn in pf_enriched[is_filled]:\n",
    "                        if pn not in repeated[is_filled]:\n",
    "                            repeated[is_filled][pn] = [pf_enriched[is_filled][pn]]\n",
    "                        repeated[is_filled][pn].append((verb, vvals))\n",
    "                    else:\n",
    "                        pf_enriched[is_filled][pn] = (verb, vvals)\n",
    "                    if F.otype.v(pn) != 'phrase': \n",
    "                        non_phrase[pn].append(verb)\n",
    "            for pn in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if pn not in vresults[False]:\n",
    "                    wrong_node[pn].append(verb)\n",
    "            for pn in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                f_corr = pn in pf_corr                               # manual correction in phrase function\n",
    "                f_good = pf_corr.get(pn, F.function.v(pn))\n",
    "                s_manual = pn in vresults[True] and vresults[False][pn] != vresults[True][pn] # real change\n",
    "                these_results = vresults[True][pn] if s_manual else vresults[False][pn]\n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((pn,)+these_results+(f_good, f_corr, s_manual))\n",
    "                results.append((pn,)+these_results+(f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                msg('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), withtime=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    msg('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), withtime=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        else:\n",
    "            inf('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        msg('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), withtime=False)\n",
    "                        for vals in overview:\n",
    "                            msg('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), withtime=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        inf('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), withtime=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        if nerrors == 0:\n",
    "            inf('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Enrichments have been applied to non-phrase nodes:')\n",
    "        for x in sorted(non_phrase)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} Node {} is not a phrase but a {}'.format(\n",
    "                non_phrase[x], T.passage(x), x, F.otype.v(x),\n",
    "            ), withtime=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        msg('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} node {}'.format(\n",
    "                non_phrase[x], T.passage(x), x,\n",
    "            ), withtime=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        inf('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "            inf('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                T.passage(x), x, ','.join(vals), px, cx\n",
    "            ), withtime=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: inf('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        msg('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-06'},\n",
    "    [\n",
    "        (None, 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in list(enrich_fields.keys())+['function', 'f_correction', 's_manual']\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Annox complements\n",
    "We load the new and modified features into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in ``enrich_fields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: etcbc4b: UP TO DATE\n",
      "  0.00s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.03s DETAIL: load main: X. [node]  -> \n",
      "  1.51s DETAIL: load main: X. [e]  -> \n",
      "  3.71s DETAIL: load main: G.node_anchor_min\n",
      "  3.81s DETAIL: load main: G.node_anchor_max\n",
      "  3.90s DETAIL: load main: G.node_sort\n",
      "  4.00s DETAIL: load main: G.node_sort_inv\n",
      "  4.73s DETAIL: load main: G.edges_from\n",
      "  4.85s DETAIL: load main: G.edges_to\n",
      "  4.99s LOGFILE=/Users/dirk/laf/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  4.99s PARSING ANNOTATION FILES\n",
      "  5.12s INFO: parsing complements.xml\n",
      "  8.49s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "     52300 good   annots   and     0 faulty ones\n",
      "    470700 good   features and     0 faulty ones\n",
      "     52300 distinct xml identifiers\n",
      "\n",
      "  8.49s MODELING RESULT FILES\n",
      "  8.50s INFO: CONNECTIVITY\n",
      "  8.64s WRITING RESULT FILES for a: complements\n",
      "  8.65s DETAIL: write annox complements: F.JanetDyk_ft_f_correction [node] \n",
      "  8.71s DETAIL: write annox complements: F.JanetDyk_ft_function [node] \n",
      "  8.79s DETAIL: write annox complements: F.JanetDyk_ft_grammatical [node] \n",
      "  8.85s DETAIL: write annox complements: F.JanetDyk_ft_lexical [node] \n",
      "  8.90s DETAIL: write annox complements: F.JanetDyk_ft_original [node] \n",
      "  8.93s DETAIL: write annox complements: F.JanetDyk_ft_predication [node] \n",
      "  9.00s DETAIL: write annox complements: F.JanetDyk_ft_s_manual [node] \n",
      "  9.06s DETAIL: write annox complements: F.JanetDyk_ft_semantic [node] \n",
      "  9.11s DETAIL: write annox complements: F.JanetDyk_ft_valence [node] \n",
      "  9.18s END   COMPILE a: complements\n",
      "  9.18s USING annox: complements DATA COMPILED AT: 2016-08-25T12-59-29\n",
      "  9.19s INFO: Feature function may mean any of etcbc4:ft.function, JanetDyk:ft.function. Choosing JanetDyk:ft.function\n",
      "  9.19s DETAIL: keep main: G.node_anchor_min\n",
      "  9.19s DETAIL: keep main: G.node_anchor_max\n",
      "  9.19s DETAIL: keep main: G.node_sort\n",
      "  9.19s DETAIL: keep main: G.node_sort_inv\n",
      "  9.19s DETAIL: keep main: G.edges_from\n",
      "  9.19s DETAIL: keep main: G.edges_to\n",
      "  9.19s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  9.19s DETAIL: keep main: F.etcbc4_ft_lex [node] \n",
      "  9.19s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  9.19s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_g_lex [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_g_word [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  9.19s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  9.19s DETAIL: clear annox lexicon: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  9.19s DETAIL: clear annox lexicon: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  9.19s DETAIL: clear annox lexicon: F.etcbc4_ph_phono [node] \n",
      "  9.21s DETAIL: clear annox lexicon: F.etcbc4_ph_phono_sep [node] \n",
      "  9.22s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "    10s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "    10s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_f_correction [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_function [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_grammatical [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_lexical [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_original [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_predication [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_s_manual [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_semantic [node] \n",
      "    11s DETAIL: load annox complements: F.JanetDyk_ft_valence [node] \n",
      "    11s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    11s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    11s INFO: LOADING PREPARED data: please wait ... \n",
      "    11s DETAIL: keep prep: G.node_sort\n",
      "    11s DETAIL: keep prep: G.node_sort_inv\n",
      "    11s DETAIL: keep prep: L.node_up\n",
      "    11s DETAIL: keep prep: L.node_down\n",
      "    11s DETAIL: keep prep: V.verses\n",
      "    11s DETAIL: keep prep: V.books_la\n",
      "    11s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    12s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    12s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    16s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    16s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    16s INFO: LOADED PREPARED data\n",
      "    16s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX complements, lexicon FOR TASK flow_corr AT 2016-08-25T12-59-35\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        chapter verse\n",
    "        function JanetDyk:ft.function\n",
    "        s_manual f_correction\n",
    "    ''' + ' '.join(enrich_fields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n",
      "Pred - None - False\n",
      "PreC - None - False\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s('phrase'))[0:10]: \n",
    "    print('{} - {} - {}'.format(\n",
    "        F.function.v(i), \n",
    "        F.JanetDyk_ft_function.v(i),\n",
    "        L.u('clause', i) in clause_verb_selected,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single csv file for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11s collecting phrases ...\n",
      "    12s  10000 phrases in  2910 clauses ...\n",
      "    12s  20000 phrases in  5916 clauses ...\n",
      "    13s  30000 phrases in  9055 clauses ...\n",
      "    14s  40000 phrases in 12168 clauses ...\n",
      "    15s  50000 phrases in 15374 clauses ...\n",
      "    15s  52300 phrases in 16053 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = open(all_results, 'w')\n",
    "NALLFIELDS = 12\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "inf('collecting phrases ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'valence',\n",
    "    'predication',\n",
    "    'grammatical',\n",
    "    'original',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'phrase text',\n",
    "    'phrase node',\n",
    "))\n",
    "i = 0\n",
    "j = 0\n",
    "c = 0\n",
    "CHUNK_SIZE = 10000\n",
    "for cn in sorted(clause_verb_selected):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb_selected[cn])\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        T.passage(cn),\n",
    "        ' '.join(F.lex.v(verb) for verb in vrbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.words(L.d('word', cn), fmt='ec').replace('\\n', ' '),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d('phrase', cn):\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            inf('{:>6} phrases in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'True' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'True' else '',\n",
    "            T.passage(pn),\n",
    "            '',\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.words(L.d('word', pn), fmt='ec').replace('\\n', ' '),\n",
    "            pn,\n",
    "        ))\n",
    "f.close()\n",
    "inf('{:>6} phrases in {:>5} clauses done'.format(i, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
