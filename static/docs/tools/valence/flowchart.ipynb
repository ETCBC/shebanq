{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-small.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/DANS-small.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-small.png\"/></a>\n",
    "<a href=\"https://www.academic-bible.com/en/online-bibles/biblia-hebraica-stuttgartensia-bhs/read-the-bible-text/\" target=\"_blank\"><img align=\"right\" src=\"files/images/DBG-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbal valence\n",
    "\n",
    "*Verbal valence* is a kind of signature of a verb, not unlike overloading in programming languages.\n",
    "The meaning of a verb depends on the number and kind of its complements, i.e. the linguistic entities that act as arguments for the semantic function of the verb.\n",
    "\n",
    "We will use a set of flowcharts to specify and compute the sense of a verb in specific contexts depending on the verbal valence. The flowcharts have been composed by Janet Dyk. Although they are not difficult to understand, it takes a good deal of ingenuity to apply them in all the real world situations that we encounter in our corpus.\n",
    "\n",
    "\n",
    "# Authors\n",
    "\n",
    "This notebook is being written by [Dirk Roorda](dirk.roorda@dans.knaw.nl) following the ideas of \n",
    "[Janet Dyk](j.w.dyk@vu.nl). Janet's ideas have been published in various ways, see the references below.\n",
    "They can be summarized as a set of flowcharts. Each flowchart describes set of rules how to choose between\n",
    "the senses of a specific verb based on the constituents in each context where it occurs.\n",
    "The role of Dirk is to turn those ideas into a working program based on the ETCBC data.\n",
    "\n",
    "# About\n",
    "\n",
    "This is an [Jupyter](http://jupyter.org) notebook. It contains a working program to carry out the computations\n",
    "that we need for making use of verbal valence patterns.\n",
    "You can download this notebook and run it on your computer, provided you have\n",
    "[LAF-Fabric](http://laf-fabric.readthedocs.org/en/latest/texts/welcome.html) installed.\n",
    "\n",
    "There is not only code in this notebook, but also extensive documentation, and a description how to view\n",
    "the results on \n",
    "[SHEBANQ](https://shebanq.ancient-data.org) as a set of *Notes*.\n",
    "See the end of the notebook for precise links.\n",
    "\n",
    "# Status\n",
    "\n",
    "**Last modified: 2016-07-07**\n",
    "\n",
    "This notebook is not yet finished. \n",
    "It turns out that the ETCBC data at present does not contain all bits and pieces that are needed to follow\n",
    "the rules in Janet's flowcharts. It is difficult to find all direct objects, especially implied ones.\n",
    "And there are many cases where the database encodes a phrase as a complement, where the flowchart expects it to be a direct object.\n",
    "\n",
    "We have set up a workflow for correcting and enriching the ETCBC data. See the\n",
    "[corr_enrich notebook](corr_enrich.ipynb).\n",
    "There we take care that all relevant phrases get there proper *function*\n",
    "labels. And we analyse those phrases and assign new properties to them, based on certain heuristics.\n",
    "\n",
    "This flowchart notebook takes those new properties as input for determining the valencies of verbs.\n",
    " \n",
    "# More about flowcharts\n",
    "\n",
    "Here is an original flowchart by Janet, the one for NTN (*give*).\n",
    "\n",
    "<img src=\"images/FlowChartNTN-orig.pdf\"/>\n",
    "\n",
    "In order to run the flowcharts, preliminary work has to be done. \n",
    "We have to \n",
    "\n",
    "* identify direct objects;\n",
    "* divide them into principal and secundary ones if there are multiple;\n",
    "* identify complements;\n",
    "* divide them into locatives, indirect objects, and other complements;\n",
    "* detect relativa and offer them as potential direct objects;\n",
    "* detect phrases starting with MN (*from*) and offer them as potential direct objects.\n",
    "\n",
    "These are exactly the things that we outsource to the \n",
    "[corr_enrich notebook](corr_enrich.ipynb).\n",
    "\n",
    "\n",
    "# Generic flowchart\n",
    "\n",
    "The generic flowchart rules can be read off this diagram.\n",
    "\n",
    "<img src=\"images/Valence-Generic.pdf\"/>\n",
    "\n",
    "In fact, this part of the flowchart requires the most programming effort.\n",
    "\n",
    "# Specific flowcharts\n",
    "\n",
    "Using the generic flowchart, we state the rules for individual verbs, which can be expressed as simple\n",
    "multiple choice lists. Far below in this notebook, these rules will be applied to all clauses.\n",
    "\n",
    "As an example, this is a simplified flowchart for NTN in diagram form as we will implement it below.\n",
    "\n",
    "<img src=\"images/Valence-NTN.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart logic\n",
    "\n",
    "Here is the bare logic of the flow charts for the individual verbs.\n",
    "\n",
    "The ``senses`` data structure is a dictionary keyed by verb lexemes. \n",
    "For each verb it is keyed by *sense labels*, which is a code for the number of direct objects and the nature of complements that are present in the context.\n",
    "\n",
    "Behind each sense label there is information about the meaning of the verb in such a context.\n",
    "The meaning consists of 2 or 3 pieces of information.\n",
    "\n",
    "The important part is the second one, the *sense template*, which consist of a gloss augmented with placeholders for the direct objecs and complements.\n",
    "\n",
    "* **{verb}** the verb occurrence in question\n",
    "* **{dos} ** direct objects\n",
    "* **{pdos}** principal direct objects\n",
    "* **{sdos}** secundary direct objects\n",
    "* **{inds}** indirect objects\n",
    "* **{locs}** locatives\n",
    "* **{cpls}** complements, not marked as either indirect object or locative\n",
    "\n",
    "In case there are multiple entities, the algorithm returns them chunked as phrases/clauses.\n",
    "\n",
    "Apart from the template, there is also a *status* and an optional *account*. \n",
    "\n",
    "The status is ``!`` in normal cases, ``?`` in dubious cases, and ``-`` in erroneous cases.\n",
    "In SHEBANQ these statuses are translated into colors of the notes (blue/orange/red).\n",
    "\n",
    "The account contains information about the grounds of which the algorithm has arrived at its conclusions.\n",
    "\n",
    "A typical case is ``NTN[`` sense ``0c``. This verbs prefers indirect objects and not locatives.\n",
    "So when the context has a complement that fails to be classified beforehand as either locative or indirect object, this is the moment that we finally decide it is an indirect object after all.\n",
    "But this is risky, so we give it status ``?`` and we tell the user that we have decided to change ``C`` into ``I`` for this complement.\n",
    "\n",
    "Likewise, sense ``0l`` is not expected to occur. When we encounter it, we conclude that our heuristic for choosing between ``L`` and ``I`` has failed here, and we overrule that decision and change ``L`` to ``I``.\n",
    "We tell the user that here we have encountered an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senses_spec = '''\n",
    "<FH\n",
    "00:!: act; take action\n",
    "0i:?: act; take action for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: act; take action at {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: do; make; perform; observe {cpls} :: {cpls} taken as direct object\n",
    "10:!: do; make; perform; observe {dos}\n",
    "1i:?: do; make; perform; observe {dos} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:?: do; make; perform; observe {dos} at {locs} :: {locs} taken as locative adjunct\n",
    "1c:?: make {dos} to be {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: make {pdos} to be {sdos}\n",
    "\n",
    "BR>\n",
    "00:-: !not encountered!\n",
    "0i:?: create for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: create at {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: create {cpls} :: {cpls} taken as direct object\n",
    "10:!: create {dos}\n",
    "1i:?: create {dos} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:?: create {dos} at {locs} :: {locs} taken as locative adjunct\n",
    "1c:?: create {dos} to be {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: create {pdos} to be {sdos}\n",
    "\n",
    "CJT\n",
    "00:-: !not encountered!\n",
    "0i:-: !not encountered!\n",
    "0l:-: !not encountered!\n",
    "0c:?: install; set up; put in place {cpls} :: {cpls} taken as direct object\n",
    "10:!: install; set up; put in place {dos}\n",
    "1i:?: place {dos} for the benefit of {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:!: place {dos} ... {locs}\n",
    "1c:?: make {dos} to be {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: make {pdos} to be {sdos}\n",
    "\n",
    "DBQ\n",
    "00:-: !not encountered!\n",
    "0i:?: cling; cleave; adhere to {inds} :: {inds} taken as locative\n",
    "0l:!: cling; cleave; adhere after/to {locs}\n",
    "0c:?: cling; cleave; adhere to {cpls} :: {cpls} taken as locative\n",
    "10:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1i:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1l:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1c:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "2 :-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "\n",
    "FJM\n",
    "00:!: prepare; put in place; make ready\n",
    "0i:?: prepare; put in place; make ready for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:!: make ready; prepare {locs} (specific meaning depending on preposition)\n",
    "0c:?: prepare; put in place; institute {pdos} :: {cpls} taken as extra direct object besides {pdos}\n",
    "10:!: prepare; put in place; institute {dos}\n",
    "1i:?: prepare; put in place; institute {dos} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:!: put; place {dos} ... {locs} (specific meaning depending on preposition)\n",
    "1c:?: make {dos} (to be (as)/to become/to do) {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: make {pdos} (to be (as)/to become/to do) {sdos}\n",
    "\n",
    "NTN\n",
    "00:!: (act of) producing; yielding; giving (in itself)\n",
    "0i:!: produce for; yield for; give to {inds}\n",
    "0l:-: !not encountered!\n",
    "0c:?: produce; yield; give {cpls} :: {cpls} taken as extra direct object besides {pdos}\n",
    "10:!: produce; yield; give {dos}\n",
    "1i:!: give {dos} to {inds}\n",
    "1l:!: place {dos} ... {locs}\n",
    "1c:?: make {dos} (to be (as)/to become/to do) {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: make {pdos} (to be (as)/to become/to do) {sdos}\n",
    "\n",
    "QR>\n",
    "00:!: shout; call; invoke\n",
    "0i:!: call; summon {inds}\n",
    "0l:?: call at {locs} :: {locs} taken as locative adjunct.\n",
    "0c:?: call {cpls} (content) :: {cpls} taken as direct object\n",
    "10:!: call; summon {dos} (content or addressee)\n",
    "1i:!: summon {dos} for {inds}\n",
    "1l:!: call out {dos} before {locs}\n",
    "1c:?: call {dos} (to be named) {cpls} :: {cpls} taken as extra direct object besides {dos}\n",
    "2 :!: call {pdos} (to be named) {sdos}\n",
    "\n",
    "ZQN\n",
    "00:!: be old\n",
    "0i:?: be old for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: be old in {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: be old ... {cpls} :: {cpls} taken as adjunct\n",
    "10:-: !not encountered!\n",
    "1i:-: !not encountered!\n",
    "1l:-: !not encountered!\n",
    "1c:-: !not encountered!\n",
    "2 :-: !not encountered!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "See the results on SHEBANQ.\n",
    "\n",
    "The complete set of results is in the note set \n",
    "[valence](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxlbmNl&tp=txt_tb1).\n",
    "You can find it on the Notes page in SHEBANQ:\n",
    "\n",
    "<img src=\"images/valnotes.png\"/>\n",
    "\n",
    "By checking the other note sets you *mute* them, so they do not show up among the lines.\n",
    "\n",
    "In order to see a note set, click on its name. You then go to pages with all verses that have a note of this set attached. \n",
    "\n",
    "<img src=\"images/notesview.png\"/>\n",
    "\n",
    "In order to see the actual notes, click the comment cloud icons. If you click the upper left one, notes are fetched for all verses on the page.\n",
    "\n",
    "<img src=\"images/withnotes.png\"/>\n",
    "\n",
    "You can also export the notes as csv, or view them in a chart.\n",
    "\n",
    "The *valence* set has the following subsets:\n",
    "\n",
    "* Unresolved results: [val_nb](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfbmI_&tp=txt_tb1);\n",
    "* Uncertain results: [val_wrn](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfd3Ju&tp=txt_tb1);\n",
    "* Erroneous results: [val_err](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfZXJy&tp=txt_tb1);\n",
    "* Promotion candidates [val_prom](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfcHJvbQ__&tp=txt_tb1)\n",
    "\n",
    "So if you follow the *valence* link you see them all, but you can also focus on the problematic cases.\n",
    "\n",
    "And if you are logged in, you can add remarks in free text. Just start typing in one of the new note boxes.\n",
    "Hint: use the keyword **val_note** for your manual notes to valence, then other users can see all relevant information about valence together.\n",
    "\n",
    "By clicking on the status symbol you can cycle through different display styles and colors for your note.\n",
    "Do not forget to save when you are done!\n",
    "\n",
    "See also the SHEBANQ help on notes:\n",
    "[general](https://shebanq.ancient-data.org/help#notes)\n",
    "[notes view](https://shebanq.ancient-data.org/help#notes_style)\n",
    "[working with notes](https://shebanq.ancient-data.org/help#working_with_notes)\n",
    "\n",
    "If you have a solid contribution to make, e.g. the outcome of an algorithm, consider\n",
    "[bulk uploading notes](https://shebanq.ancient-data.org/help#bulk_uploading_notes).\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext (author's copy with deviant page numbering)](https://shebanq.ancient-data.org/static/docs/methods/2014_Dyk_jnsl.pdf)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firing up the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.8.3\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.01s DETAIL: COMPILING m: etcbc4b: UP TO DATE\n",
      "  0.01s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.01s DETAIL: COMPILING a: complements: UP TO DATE\n",
      "  0.01s USING annox: complements DATA COMPILED AT: 2016-10-26T17-57-50\n",
      "  0.01s DETAIL: COMPILING a: lexicon: UP TO DATE\n",
      "  0.01s USING annox: lexicon DATA COMPILED AT: 2016-07-08T14-32-54\n",
      "  0.02s DETAIL: load main: G.node_anchor_min\n",
      "  0.14s DETAIL: load main: G.node_anchor_max\n",
      "  0.22s DETAIL: load main: G.node_sort\n",
      "  0.28s DETAIL: load main: G.node_sort_inv\n",
      "  0.69s DETAIL: load main: G.edges_from\n",
      "  0.75s DETAIL: load main: G.edges_to\n",
      "  0.81s DETAIL: load main: F.etcbc4_db_monads [node] \n",
      "  2.40s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  3.57s DETAIL: load main: F.etcbc4_db_otype [node] \n",
      "  4.27s DETAIL: load main: F.etcbc4_ft_det [node] \n",
      "  4.47s DETAIL: load main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  4.79s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  4.99s DETAIL: load main: F.etcbc4_ft_ls [node] \n",
      "  5.18s DETAIL: load main: F.etcbc4_ft_number [node] \n",
      "  5.90s DETAIL: load main: F.etcbc4_ft_prs [node] \n",
      "  6.16s DETAIL: load main: F.etcbc4_ft_rela [node] \n",
      "  6.51s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  6.71s DETAIL: load main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  6.84s DETAIL: load main: F.etcbc4_ft_uvf [node] \n",
      "  7.06s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  7.25s DETAIL: load main: F.etcbc4_ft_vt [node] \n",
      "  7.46s DETAIL: load main: F.etcbc4_sft_book [node] \n",
      "  7.48s DETAIL: load main: F.etcbc4_sft_chapter [node] \n",
      "  7.50s DETAIL: load main: F.etcbc4_sft_label [node] \n",
      "  7.54s DETAIL: load main: F.etcbc4_sft_verse [node] \n",
      "  7.56s DETAIL: load main: F.etcbc4_ft_mother [e] \n",
      "  7.64s DETAIL: load main: C.etcbc4_ft_mother -> \n",
      "  7.82s DETAIL: load main: C.etcbc4_ft_mother <- \n",
      "  7.97s DETAIL: load annox lexicon: F.etcbc4_lex_gloss [node] \n",
      "  8.19s DETAIL: load annox lexicon: F.etcbc4_lex_nametype [node] \n",
      "  8.31s DETAIL: load annox complements: F.JanetDyk_ft_f_correction [node] \n",
      "  8.37s DETAIL: load annox complements: F.JanetDyk_ft_function [node] \n",
      "  8.41s DETAIL: load annox complements: F.JanetDyk_ft_grammatical [node] \n",
      "  8.45s DETAIL: load annox complements: F.JanetDyk_ft_lexical [node] \n",
      "  8.48s DETAIL: load annox complements: F.JanetDyk_ft_original [node] \n",
      "  8.50s DETAIL: load annox complements: F.JanetDyk_ft_predication [node] \n",
      "  8.55s DETAIL: load annox complements: F.JanetDyk_ft_s_manual [node] \n",
      "  8.58s DETAIL: load annox complements: F.JanetDyk_ft_semantic [node] \n",
      "  8.63s DETAIL: load annox complements: F.JanetDyk_ft_valence [node] \n",
      "  8.67s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/valence/__log__valence.txt\n",
      "  8.78s INFO: LOADING PREPARED data: please wait ... \n",
      "  8.78s prep prep: G.node_sort\n",
      "  8.85s prep prep: G.node_sort_inv\n",
      "    10s prep prep: L.node_up\n",
      "    14s prep prep: L.node_down\n",
      "    20s prep prep: V.verses\n",
      "    20s prep prep: V.books_la\n",
      "    20s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    22s INFO: LOADED PREPARED data\n",
      "    22s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon, complements FOR TASK valence AT 2016-10-26T18-16-32\n"
     ]
    }
   ],
   "source": [
    "version = '4b'\n",
    "API = fabric.load('etcbc{}'.format(version), 'lexicon,complements', 'valence', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads\n",
    "        JanetDyk:ft.function rela\n",
    "        g_word_utf8 trailer_utf8\n",
    "        lex prs uvf sp ls vs vt nametype det gloss\n",
    "        book chapter verse label number\n",
    "        s_manual f_correction\n",
    "        valence predication grammatical original lexical semantic\n",
    "    ''',\n",
    "    '''\n",
    "        mother\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "    \"primary\": False,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "result_dir = '{}/results'.format(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicators\n",
    "\n",
    "Here we specify by what features we recognize key constituents.\n",
    "We use predominantly features that come from the correction/enrichment workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pf ... : predication feature\n",
    "# gf_... : grammatical feature\n",
    "# vf_... : valence feature\n",
    "# sf_... : lexical feature\n",
    "# of_... : original feature\n",
    "\n",
    "pf_predicate = {\n",
    "    'regular',\n",
    "    'copula',\n",
    "}\n",
    "gf_direct_object = {\n",
    "    'principal_direct_object',\n",
    "    'direct_object',\n",
    "}\n",
    "gf_principal_do = {\n",
    "    'principal_direct_object',\n",
    "}\n",
    "gf_indirect_object = {\n",
    "    'indirect_object',\n",
    "}\n",
    "gf_complement = {\n",
    "    '*',\n",
    "}\n",
    "sf_locative = {\n",
    "    'location',\n",
    "}\n",
    "vf_locative = {\n",
    "    'complement',\n",
    "    'adjunct',\n",
    "}\n",
    "\n",
    "to_be = set('''\n",
    "    HJH[ HWH[\n",
    "'''.strip().split())\n",
    "\n",
    "verbal_stems = set('''\n",
    "    qal\n",
    "'''.strip().split())\n",
    "\n",
    "pronominal_suffix = {\n",
    "    'W': ('p3-sg-m', 'him'),\n",
    "    'K': ('p2-sg-m', 'you:m'),\n",
    "    'J': ('p1-sg-', 'me'),\n",
    "    'M': ('p3-pl-m', 'them:mm'),\n",
    "    'H': ('p3-sg-f', 'her'),\n",
    "    'HM': ('p3-pl-m', 'them:mm'),\n",
    "    'KM': ('p2-pl-m', 'you:mm'),\n",
    "    'NW': ('p1-pl-', 'us'),\n",
    "    'HW': ('p3-sg-m', 'him'),\n",
    "    'NJ': ('p1-sg-', 'me'),\n",
    "    'K=': ('p2-sg-f', 'you:f'),\n",
    "    'HN': ('p3-pl-f', 'them:ff'),\n",
    "    'MW': ('p3-pl-m', 'them:mm'),\n",
    "    'N': ('p3-pl-f', 'them:ff'),\n",
    "    'KN': ('p2-pl-f', 'you:ff'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a verb-clause index\n",
    "\n",
    "We generate an index which gives for each verb lexeme a list of clauses that have that lexeme as the main verb.\n",
    "In the index we store the clause node together with the word node(s) that carries the main verb(s).\n",
    "\n",
    "Clauses may have multiple verbs. In many cases it is 'HJH[' (or 'HWH[') plus an other verb.\n",
    "In those cases, it is the other verb that is the main verb.\n",
    "\n",
    "Yet, there are also sentences with more than one main verb.\n",
    "In those cases, we treat both verbs separately as main verb of one and the same clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    16s Making the verb-clause index\n",
      "    18s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 multiple verb clauses of total 88011 clauses\n",
      "Genesis 8:5#9_1 HLK[ XSR[\n",
      "Sacharia 8:10#25_1 JY>[ BW>[\n",
      "Chronica_II 15:5#14_1 JY>[ BW>[\n"
     ]
    }
   ],
   "source": [
    "msg('Making the verb-clause index')\n",
    "nclauses = 0\n",
    "multiple = []\n",
    "verb_clause = collections.defaultdict(lambda: [])\n",
    "clause_verb = collections.OrderedDict()\n",
    "\n",
    "for c in F.otype.s('clause'):\n",
    "    nclauses += 1\n",
    "    the_verbs = []\n",
    "    for p in L.d('phrase', c):\n",
    "        pf = F.predication.v(p)\n",
    "        if pf in pf_predicate:\n",
    "            for w in L.d('word', p):\n",
    "                if F.sp.v(w) == 'verb': the_verbs.append(w)\n",
    "    if len(the_verbs):\n",
    "        real_verbs = []\n",
    "        keep_to_be = len(the_verbs) == 1\n",
    "        for v in the_verbs:\n",
    "            vl = F.lex.v(v)\n",
    "            if keep_to_be or (vl not in to_be): real_verbs.append(v)\n",
    "        if len(real_verbs) > 1: multiple.append('{} {}:{}#{}_{} {}'.format(\n",
    "            F.book.v(L.u('book', v)),\n",
    "            F.chapter.v(L.u('chapter', v)),\n",
    "            F.verse.v(L.u('verse', v)),\n",
    "            F.number.v(L.u('sentence', v)),\n",
    "            F.number.v(c),\n",
    "            ' '.join(F.lex.v(x) for x in real_verbs),\n",
    "        ))\n",
    "        for v in real_verbs:\n",
    "            vl = F.lex.v(v)\n",
    "            verb_clause[vl].append((c,v))\n",
    "        if len(real_verbs):\n",
    "            clause_verb[c] = tuple(real_verbs)\n",
    "msg('Done')\n",
    "print('There are {} multiple verb clauses of total {} clauses'.format(len(multiple), nclauses))\n",
    "print('\\n'.join(multiple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Indirect) Objects, Locatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10m 08s Finding key constituents\n",
      "10m 11s Done\n"
     ]
    }
   ],
   "source": [
    "msg('Finding key constituents')\n",
    "directobjects = {}\n",
    "principal_dos = {}\n",
    "secondary_dos = {}\n",
    "cast_constituents = {}\n",
    "indirectobjects = {}\n",
    "locatives = {}\n",
    "complements = {}\n",
    "\n",
    "# go through all clauses and collect all types of direct objects\n",
    "for c in F.otype.s('clause'): \n",
    "    # phrase like constituents\n",
    "    directobjects[c] = set()\n",
    "    principal_dos[c] = set()\n",
    "    secondary_dos[c] = set()\n",
    "    cast_constituents[c] = set()\n",
    "    indirectobjects[c] = set()\n",
    "    locatives[c] = set()\n",
    "    complements[c] = set()\n",
    "    for p in L.d('phrase', c):\n",
    "        gf = F.grammatical.v(p)\n",
    "        of = F.original.v(p)\n",
    "        sf = F.semantic.v(p)\n",
    "        vf = F.valence.v(p)\n",
    "        if gf in gf_direct_object:\n",
    "            directobjects[c].add(p)\n",
    "        if gf in gf_principal_do:\n",
    "            principal_dos[c].add(p)\n",
    "        if gf in gf_indirect_object:\n",
    "            indirectobjects[c].add(p)\n",
    "        if gf in gf_complement:\n",
    "            complements[c].add(p)\n",
    "        if sf in sf_locative and vf in vf_locative:\n",
    "            locatives[c].add(p)\n",
    "        if of :\n",
    "            cast_constituents[c].add(p)\n",
    "\n",
    "    # clause like constituents: only look for object clauses dependent on this clause\n",
    "    for ac in L.d('clause', L.u('sentence', c)):\n",
    "        dep = list(C.mother.v(ac))\n",
    "        if len(dep) and dep[0] == c:\n",
    "            gf = F.grammatical.v(p)\n",
    "            if gf in gf_direct_object:\n",
    "                directobjects[c].add(p)\n",
    "\n",
    "    # compute secondary objects, i.e. direct objects minus the principal one if there is one\n",
    "    secondary_dos[c] = directobjects[c] - principal_dos[c]\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    # dobjects_order = sorted(dobjects_set, key=NK)\n",
    "\n",
    "# NB: the map directobjects has as values sets of nodes.\n",
    "# These nodes can be phrases or clauses.\n",
    "msg('Done') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new function = Adju; text=Adju\n",
      "valence = adjunct; grammatical = NA; lexical = ; semantic = \n",
      "clause: וְיָשֵׂ֥ם לְךָ֖ שָׁלֹֽום׃ ס \n",
      "\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "testp = 647761\n",
    "print('new function = {}; text={}'.format(\n",
    "        F.function.v(testp), F.JanetDyk_ft_function.v(testp), T.words(L.d('word', testp),\n",
    ")))\n",
    "print('valence = {}; grammatical = {}; lexical = {}; semantic = {}'.format(\n",
    "    F.valence.v(testp),\n",
    "    F.grammatical.v(testp),\n",
    "    F.lexical.v(testp),\n",
    "    F.semantic.v(testp),\n",
    "))\n",
    "testc = 440568\n",
    "print('{}: {}'.format(F.otype.v(testc), T.words(L.d('word', testc))))\n",
    "\n",
    "print(complements[testc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_clauses = len(list(F.otype.s('clause')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 88011 clauses in total\n",
      "direct objects\n",
      "\t 81098 clauses with  0 direct objects\n",
      "\t  5850 clauses with  1 direct objects\n",
      "\t  1022 clauses with  2 direct objects\n",
      "\t    41 clauses with  3 direct objects\n",
      "\t 88011 clauses\n",
      "clause objects\n",
      "\t 88011 clauses with  0 clause objects\n",
      "\t 88011 clauses\n",
      "principal objects\n",
      "\t 86488 clauses with  0 principal objects\n",
      "\t  1523 clauses with  1 principal objects\n",
      "\t 88011 clauses\n",
      "secondary objects\n",
      "\t 81558 clauses with  0 secondary objects\n",
      "\t  6412 clauses with  1 secondary objects\n",
      "\t    41 clauses with  2 secondary objects\n",
      "\t 88011 clauses\n",
      "indirect objects\n",
      "\t 86635 clauses with  0 indirect objects\n",
      "\t  1373 clauses with  1 indirect objects\n",
      "\t     3 clauses with  2 indirect objects\n",
      "\t 88011 clauses\n",
      "complements\n",
      "\t 81802 clauses with  0 complements\n",
      "\t  5979 clauses with  1 complements\n",
      "\t   223 clauses with  2 complements\n",
      "\t     5 clauses with  3 complements\n",
      "\t     2 clauses with  4 complements\n",
      "\t 88011 clauses\n",
      "locatives\n",
      "\t 83774 clauses with  0 locatives\n",
      "\t  4096 clauses with  1 locatives\n",
      "\t   129 clauses with  2 locatives\n",
      "\t    12 clauses with  3 locatives\n",
      "\t 88011 clauses\n",
      "cast constituents\n",
      "\t 86883 clauses with  0 cast constituents\n",
      "\t  1107 clauses with  1 cast constituents\n",
      "\t    21 clauses with  2 cast constituents\n",
      "\t 88011 clauses\n"
     ]
    }
   ],
   "source": [
    "# Counting constituents\n",
    "cnt_directobjects = collections.Counter()\n",
    "cnt_clauseobjects = collections.Counter()\n",
    "cnt_principal_dos = collections.Counter()\n",
    "cnt_secondary_dos = collections.Counter()\n",
    "cnt_indirectobjects = collections.Counter()\n",
    "cnt_complements = collections.Counter()\n",
    "cnt_locatives = collections.Counter()\n",
    "cnt_cast_constituents = collections.Counter()\n",
    "\n",
    "for (c, xs) in directobjects.items(): \n",
    "    cnt_directobjects[len(xs)] += 1\n",
    "    nco = len({x for x in xs if F.otype.v(x) == 'clause'})\n",
    "    cnt_clauseobjects[nco] += 1\n",
    "for (c, xs) in principal_dos.items(): cnt_principal_dos[len(xs)] += 1\n",
    "for (c, xs) in secondary_dos.items(): cnt_secondary_dos[len(xs)] += 1\n",
    "for (c, xs) in indirectobjects.items(): cnt_indirectobjects[len(xs)] += 1\n",
    "for (c, xs) in complements.items(): cnt_complements[len(xs)] += 1\n",
    "for (c, xs) in locatives.items(): cnt_locatives[len(xs)] += 1\n",
    "for (c, xs) in cast_constituents.items(): cnt_cast_constituents[len(xs)] += 1\n",
    "\n",
    "print('\\t {:>5} clauses in total'.format(total_clauses))\n",
    "for (label, cnt_map) in (\n",
    "        ('direct objects', cnt_directobjects),\n",
    "        ('clause objects', cnt_clauseobjects),\n",
    "        ('principal objects', cnt_principal_dos),\n",
    "        ('secondary objects', cnt_secondary_dos),\n",
    "        ('indirect objects', cnt_indirectobjects),\n",
    "        ('complements', cnt_complements),\n",
    "        ('locatives', cnt_locatives),\n",
    "        ('cast constituents', cnt_cast_constituents),\n",
    "    ):\n",
    "    thistotal = 0\n",
    "    print('{}'.format(label))\n",
    "    for n in sorted(cnt_map):\n",
    "        amount = cnt_map[n]\n",
    "        thistotal += amount\n",
    "        print('\\t {:>5} clauses with {:>2} {}'.format(amount, n, label))\n",
    "    print('\\t {:>5} clauses'.format(thistotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Applying the flowchart\n",
    "\n",
    "We can now apply the flowchart in a straightforward manner.\n",
    "\n",
    "We output the results as a stand-alone comma separated file, with these columns as specified in the code below.\n",
    "This file can be used to import into a spreadsheet and check results.\n",
    "\n",
    "We also provide a comma separated file that can be imported directly into SHEBANQ as a set of notes, so that the reader can check results within SHEBANQ. This has the benefit that the full context is available, and also data view can be called up easily to inspect the coding situation for each particular instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "status_rep = {\n",
    "    '*': 'note',\n",
    "    '!': 'good',\n",
    "    '?': 'warning',\n",
    "    '-': 'error',\n",
    "}\n",
    "stat_rep = {\n",
    "    '*': 'NB',\n",
    "    '!': '',\n",
    "    '?': 'wrn',\n",
    "    '-': 'err',\n",
    "}\n",
    "\n",
    "def reptext(label, phrases, num=False, txt=False, gloss=False, textformat='ec'): \n",
    "    if phrases == None: return ''\n",
    "    label_rep = '{}='.format(label) if label else ''\n",
    "    phrases_rep = []\n",
    "    for p in sorted(phrases, key=NK):\n",
    "        ptext = '[{}|'.format(F.number.v(p) if num else '[')\n",
    "        if txt:\n",
    "            #ptext += (''.join('{}{}'.format(\n",
    "            #    F.g_word_utf8.v(w),\n",
    "            #    F.trailer_utf8.v(w),\n",
    "            #) for w in L.d('word',p ))).replace('\\n','')\n",
    "            ptext += T.words(L.d('word', p), fmt=textformat).replace('\\n', '.')\n",
    "        if gloss:\n",
    "            wtexts = []\n",
    "            for w in L.d('word',p ):\n",
    "                g = F.gloss.v(w).replace('<object marker>','&')\n",
    "                prs = F.prs.v(w)\n",
    "                prs_g = pronominal_suffix.get(prs, (None, None))[1]\n",
    "                uvf = F.uvf.v(w)\n",
    "                wtext = ''\n",
    "                if uvf == 'H': ptext += 'toward '\n",
    "                wtext += g\n",
    "                wtext += ('~'+prs_g) if prs_g != None else ''\n",
    "                wtexts.append(wtext)\n",
    "            ptext += ' '.join(wtexts)\n",
    "        ptext += ']'\n",
    "        phrases_rep.append(ptext)\n",
    "    return ' '.join(phrases_rep)\n",
    "\n",
    "def flowchart(lex, verb, dos, pdos, sdos, inds, locs, cpls):\n",
    "    sense_label = None\n",
    "    n_dos = len(dos)\n",
    "    n_pdos = len(pdos)\n",
    "    n_sdos = len(sdos)\n",
    "    n_inds = len(inds)\n",
    "    n_locs = len(locs)\n",
    "    n_cpls = len(cpls)\n",
    "    na_cpls = n_inds + n_locs + n_cpls\n",
    "    ndo = ''\n",
    "    kcp = ''\n",
    "\n",
    "    if n_dos == 0: ndo = '0'\n",
    "    elif n_dos == 1: ndo = '1'\n",
    "    else: ndo = '2'\n",
    "    \n",
    "    if na_cpls == 0: kcp = '0'\n",
    "    elif n_inds: kcp = 'i'\n",
    "    elif n_locs: kcp = 'l'\n",
    "    else: kcp = 'c'\n",
    "    sense_label = ndo+kcp if ndo != '2' else '2'\n",
    "    \n",
    "    sinfo = senses.\\\n",
    "        get(lex, {lex: {'': ('-', 'no senses given for {}'.format(lex))}}).\\\n",
    "        get(sense_label, ('-', 'no sense {} given for {}'.format(sense_label, lex)))\n",
    "    status = sinfo[0]\n",
    "    sense_fmt = sinfo[1][0]\n",
    "    action_fmt = sinfo[1][1] if len(sinfo[1]) >= 2 else ''\n",
    "    action_stat = sinfo[1][2] if len(sinfo) >= 3 else status\n",
    "\n",
    "    verb_rep = reptext('', verb, num=True, gloss=True)\n",
    "    dos_rep  = reptext('', dos,  num=True, gloss=True)\n",
    "    pdos_rep = reptext('', pdos, num=True, gloss=True)\n",
    "    sdos_rep = reptext('', sdos, num=True, gloss=True)\n",
    "    inds_rep = reptext('', inds, num=True, gloss=True)\n",
    "    locs_rep = reptext('', locs, num=True, gloss=True)\n",
    "    cpls_rep = reptext('', cpls, num=True, gloss=True)\n",
    "    \n",
    "    sense_txt = sense_fmt.format(\n",
    "        verb=verb_rep, dos=dos_rep, pdos=pdos_rep, sdos=sdos_rep, inds=inds_rep, locs=locs_rep, cpls=cpls_rep,\n",
    "    )\n",
    "    action_txt = action_fmt.format(\n",
    "        verb=verb_rep, dos=dos_rep, pdos=pdos_rep, sdos=sdos_rep, inds=inds_rep, locs=locs_rep, cpls=cpls_rep,\n",
    "    )\n",
    "\n",
    "    return (sense_label, status, sense_txt, action_txt, action_stat)\n",
    "\n",
    "fields = '''\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    sentence#\n",
    "    clause#\n",
    "    lex\n",
    "    status\n",
    "    sense_label\n",
    "    sense\n",
    "    action_status\n",
    "    action\n",
    "    #dos\n",
    "    #pdos\n",
    "    #sdos\n",
    "    #inds\n",
    "    #locs\n",
    "    #cpls\n",
    "    text\n",
    "'''.strip().split()\n",
    "\n",
    "sfields = '''\n",
    "    version\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    clause_atom\n",
    "    is_shared\n",
    "    is_published\n",
    "    status\n",
    "    keywords\n",
    "    ntext\n",
    "'''.strip().split()\n",
    "\n",
    "fields_fmt = ('{};' * (len(fields) - 1)) + '{}\\n' \n",
    "sfields_fmt = ('{}\\t' * (len(sfields) - 1)) + '{}\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the flowchart\n",
    "\n",
    "The next cell finally performs all the flowchart computations for all verbs in all contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13m 12s Applying the flowchart\n",
      "13m 12s No flowchart definition for verb CLM[\n",
      "13m 12s No flowchart definition for verb XNH[\n",
      "13m 12s No flowchart definition for verb VBL[\n",
      "13m 12s No flowchart definition for verb >BD[\n",
      "13m 12s No flowchart definition for verb NYH[\n",
      "13m 12s No flowchart definition for verb MCL=[\n",
      "13m 12s No flowchart definition for verb LQX[\n",
      "13m 12s No flowchart definition for verb RXM[\n",
      "13m 12s No flowchart definition for verb XRB[\n",
      "13m 12s No flowchart definition for verb ML>[\n",
      "13m 12s No flowchart definition for verb >RH[\n",
      "13m 12s No flowchart definition for verb >KL[\n",
      "13m 12s No flowchart definition for verb CSS[\n",
      "13m 12s No flowchart definition for verb PQD[\n",
      "13m 12s No flowchart definition for verb XZH[\n",
      "13m 12s No flowchart definition for verb NCQ[\n",
      "13m 12s No flowchart definition for verb QBY[\n",
      "13m 12s No flowchart definition for verb MWL[\n",
      "13m 12s No flowchart definition for verb C<H[\n",
      "13m 12s No flowchart definition for verb BRK[\n",
      "13m 12s No flowchart definition for verb QRB[\n",
      "13m 12s No flowchart definition for verb YWH[\n",
      "13m 12s No flowchart definition for verb NWS[\n",
      "13m 12s No flowchart definition for verb CM<[\n",
      "13m 12s No flowchart definition for verb KRT[\n",
      "13m 12s No flowchart definition for verb CWB=[\n",
      "13m 12s No flowchart definition for verb NBL[\n",
      "13m 12s No flowchart definition for verb R>H[\n",
      "13m 12s No flowchart definition for verb NPL[\n",
      "13m 12s No flowchart definition for verb QDC[\n",
      "13m 12s No flowchart definition for verb SBB[\n",
      "13m 12s No flowchart definition for verb NF>[\n",
      "13m 12s No flowchart definition for verb NGD[\n",
      "13m 12s No flowchart definition for verb PG<[\n",
      "13m 12s No flowchart definition for verb XRP[\n",
      "13m 12s No flowchart definition for verb QRH[\n",
      "13m 12s No flowchart definition for verb C>L[\n",
      "13m 12s No flowchart definition for verb CQL[\n",
      "13m 12s No flowchart definition for verb <MS[\n",
      "13m 12s No flowchart definition for verb FMX[\n",
      "13m 12s No flowchart definition for verb XCB[\n",
      "13m 12s No flowchart definition for verb QBR[\n",
      "13m 12s No flowchart definition for verb NBV[\n",
      "13m 12s No flowchart definition for verb JCB[\n",
      "13m 12s No flowchart definition for verb T<H[\n",
      "13m 12s No flowchart definition for verb QWM[\n",
      "13m 12s No flowchart definition for verb CBT[\n",
      "13m 12s No flowchart definition for verb JY>[\n",
      "13m 12s No flowchart definition for verb NVP[\n",
      "13m 12s No flowchart definition for verb FKR[\n",
      "13m 12s No flowchart definition for verb MWT[\n",
      "13m 12s No flowchart definition for verb XV>[\n",
      "13m 12s No flowchart definition for verb JRD[\n",
      "13m 12s No flowchart definition for verb BR>=[\n",
      "13m 12s No flowchart definition for verb LHV[\n",
      "13m 12s No flowchart definition for verb NKH[\n",
      "13m 12s No flowchart definition for verb BW>[\n",
      "13m 12s No flowchart definition for verb <MD[\n",
      "13m 12s No flowchart definition for verb <BR=[\n",
      "13m 12s No flowchart definition for verb CLX[\n",
      "13m 12s No flowchart definition for verb VHR[\n",
      "13m 12s No flowchart definition for verb CKX[\n",
      "13m 12s No flowchart definition for verb NW<[\n",
      "13m 12s No flowchart definition for verb HJH[\n",
      "13m 12s No flowchart definition for verb BKH[\n",
      "13m 12s No flowchart definition for verb B<L[\n",
      "13m 12s No flowchart definition for verb BNH[\n",
      "13m 12s No flowchart definition for verb BR>==[\n",
      "13m 12s No flowchart definition for verb <BR[\n",
      "13m 12s No flowchart definition for verb SPQ[\n",
      "13m 12s No flowchart definition for verb PFQ[\n",
      "13m 12s No flowchart definition for verb GLH[\n",
      "13m 12s No flowchart definition for verb >MR[\n",
      "13m 12s No flowchart definition for verb NYX[\n",
      "13m 12s No flowchart definition for verb XSM[\n",
      "13m 12s No flowchart definition for verb CWB[\n",
      "13m 12s No flowchart definition for verb >BH[\n",
      "13m 12s No flowchart definition for verb QR>=[\n",
      "13m 12s No flowchart definition for verb QYP[\n",
      "13m 12s No flowchart definition for verb HLL[\n",
      "13m 12s No flowchart definition for verb CPK[\n",
      "13m 12s No flowchart definition for verb HLK[\n",
      "13m 12s No flowchart definition for verb SWR[\n",
      "13m 12s No flowchart definition for verb Y<D[\n",
      "13m 12s No flowchart definition for verb NS<[\n",
      "13m 12s No flowchart definition for verb <NH[\n",
      "13m 12s No flowchart definition for verb ZKR[\n",
      "13m 12s No flowchart definition for verb <LH[\n",
      "13m 12s No flowchart definition for verb XSR[\n",
      "13m 12s No flowchart definition for verb NWX[\n",
      "13m 12s No flowchart definition for verb CMM[\n",
      "13m 12s No flowchart definition for verb FBR[\n",
      "13m 12s No verb DBQ[ in enriched corpus\n",
      "13m 12s No verb ZQN[ in enriched corpus\n",
      "13m 15s Done\n",
      "13m 15s Computed 5725 clauses with flowchart\n",
      "13m 15s Added notes for cast constituents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action     notes: 555\n",
      "cast       notes: 1149\n",
      "valence    notes: 5725\n",
      "Total      notes: 7429\n",
      "All lexemes with flowchart specification\n",
      "     Status   good   : 5107 clauses\n",
      "     Status   error  :   63 clauses\n",
      "     Status   warning:  555 clauses\n",
      "     All status      : 5725 clauses\n",
      "     Sense    00     : 1173 clauses\n",
      "     Sense    0c     :  111 clauses\n",
      "     Sense    0i     :  264 clauses\n",
      "     Sense    0l     :  196 clauses\n",
      "     Sense    10     : 1766 clauses\n",
      "     Sense    1c     :  255 clauses\n",
      "     Sense    1i     :  389 clauses\n",
      "     Sense    1l     :  656 clauses\n",
      "     Sense    2      :  915 clauses\n",
      "     All senses     : 5725 clauses\n",
      " \n",
      "<FH[\n",
      "     Status   good   : 2348 clauses\n",
      "     Status   warning:  120 clauses\n",
      "     All status      : 2468 clauses\n",
      "     Sense    00     : 1023 clauses\n",
      "     Sense    0c     :    4 clauses\n",
      "     Sense    0l     :   50 clauses\n",
      "     Sense    10     : 1174 clauses\n",
      "     Sense    1i     :    1 clauses\n",
      "     Sense    1l     :   65 clauses\n",
      "     Sense    2      :  151 clauses\n",
      "     All senses     : 2468 clauses\n",
      " \n",
      "BR>[\n",
      "     Status   good   :   29 clauses\n",
      "     Status   error  :    4 clauses\n",
      "     Status   warning:    3 clauses\n",
      "     All status      :   36 clauses\n",
      "     Sense    00     :    4 clauses\n",
      "     Sense    10     :   24 clauses\n",
      "     Sense    1l     :    3 clauses\n",
      "     Sense    2      :    5 clauses\n",
      "     All senses     :   36 clauses\n",
      " \n",
      "CJT[\n",
      "     Status   good   :   57 clauses\n",
      "     Status   error  :    5 clauses\n",
      "     Status   warning:   20 clauses\n",
      "     All status      :   82 clauses\n",
      "     Sense    00     :    1 clauses\n",
      "     Sense    0c     :    6 clauses\n",
      "     Sense    0i     :    1 clauses\n",
      "     Sense    0l     :    3 clauses\n",
      "     Sense    10     :   12 clauses\n",
      "     Sense    1c     :   11 clauses\n",
      "     Sense    1i     :    3 clauses\n",
      "     Sense    1l     :   18 clauses\n",
      "     Sense    2      :   27 clauses\n",
      "     All senses     :   82 clauses\n",
      " \n",
      "DBQ[\n",
      "     All status      :    0 clauses\n",
      "     All senses     :    0 clauses\n",
      " \n",
      "FJM[\n",
      "     Status   good   :  446 clauses\n",
      "     Status   warning:  131 clauses\n",
      "     All status      :  577 clauses\n",
      "     Sense    00     :   11 clauses\n",
      "     Sense    0c     :   32 clauses\n",
      "     Sense    0l     :   31 clauses\n",
      "     Sense    10     :   81 clauses\n",
      "     Sense    1c     :   90 clauses\n",
      "     Sense    1i     :    9 clauses\n",
      "     Sense    1l     :  168 clauses\n",
      "     Sense    2      :  155 clauses\n",
      "     All senses     :  577 clauses\n",
      " \n",
      "NTN[\n",
      "     Status   good   : 1667 clauses\n",
      "     Status   error  :   54 clauses\n",
      "     Status   warning:  189 clauses\n",
      "     All status      : 1910 clauses\n",
      "     Sense    00     :   55 clauses\n",
      "     Sense    0c     :   45 clauses\n",
      "     Sense    0i     :  156 clauses\n",
      "     Sense    0l     :   54 clauses\n",
      "     Sense    10     :  299 clauses\n",
      "     Sense    1c     :  144 clauses\n",
      "     Sense    1i     :  336 clauses\n",
      "     Sense    1l     :  373 clauses\n",
      "     Sense    2      :  448 clauses\n",
      "     All senses     : 1910 clauses\n",
      " \n",
      "QR>[\n",
      "     Status   good   :  560 clauses\n",
      "     Status   warning:   92 clauses\n",
      "     All status      :  652 clauses\n",
      "     Sense    00     :   79 clauses\n",
      "     Sense    0c     :   24 clauses\n",
      "     Sense    0i     :  107 clauses\n",
      "     Sense    0l     :   58 clauses\n",
      "     Sense    10     :  176 clauses\n",
      "     Sense    1c     :   10 clauses\n",
      "     Sense    1i     :   40 clauses\n",
      "     Sense    1l     :   29 clauses\n",
      "     Sense    2      :  129 clauses\n",
      "     All senses     :  652 clauses\n",
      " \n",
      "ZQN[\n",
      "     All status      :    0 clauses\n",
      "     All senses     :    0 clauses\n",
      " \n"
     ]
    }
   ],
   "source": [
    "msg('Applying the flowchart')\n",
    "\n",
    "outcome_sta = collections.Counter()\n",
    "outcome_lab = collections.Counter()\n",
    "outcome_sta_l = collections.defaultdict(lambda: collections.Counter())\n",
    "outcome_lab_l = collections.defaultdict(lambda: collections.Counter())\n",
    "\n",
    "of = open('{}/{}'.format(result_dir, 'valence_results.csv'), 'w')\n",
    "ofs = open('{}/{}'.format(result_dir, 'valence_notes.csv'), 'w')\n",
    "of.write('{}\\n'.format(';'.join(fields)))\n",
    "ofs.write('{}\\n'.format('\\t'.join(sfields)))\n",
    "\n",
    "note_keyword_base = 'valence'\n",
    "\n",
    "senses = {}\n",
    "senses_blocks = senses_spec.strip().split('\\n\\n')\n",
    "for b in senses_blocks:\n",
    "    lines = b.split('\\n')\n",
    "    verb = lines[0]+'['\n",
    "    sense_parts = [l.split(':', 2) for l in lines[1:]]\n",
    "    senses[verb] = dict(\n",
    "        (x[0].strip(), (x[1].strip(), [y.strip() for y in x[2].strip().split('::')])) for x in sense_parts\n",
    "    )\n",
    "\n",
    "nnotes = collections.Counter()\n",
    "\n",
    "for lex in verb_clause:\n",
    "    if lex not in senses:\n",
    "        msg('No flowchart definition for verb {}'.format(lex))\n",
    "for lex in senses:\n",
    "    if lex not in verb_clause:\n",
    "        msg('No verb {} in enriched corpus'.format(lex))\n",
    "        continue\n",
    "    for (c,v) in verb_clause[lex]:\n",
    "        if F.vs.v(v) not in verbal_stems: continue\n",
    "    \n",
    "        book = F.book.v(L.u('book', v))\n",
    "        chapter = F.chapter.v(L.u('chapter', v))\n",
    "        verse = F.verse.v(L.u('verse', v))\n",
    "        sentence_n = F.number.v(L.u('sentence', v))\n",
    "        clause_n = F.number.v(c)\n",
    "        clause_atom_n = F.number.v(L.u('clause_atom', v))\n",
    "        \n",
    "        verb = [L.u('phrase', v)]\n",
    "        dos = directobjects[c]\n",
    "        pdos = principal_dos[c]\n",
    "        sdos = secondary_dos[c]\n",
    "        inds = indirectobjects[c]\n",
    "        locs = locatives[c]\n",
    "        cpls = complements[c]\n",
    "        \n",
    "        (sense_label, status, sense_txt, action_txt, action_stat) = flowchart(\n",
    "            lex, verb, dos, pdos, sdos, inds, locs, cpls,\n",
    "        )\n",
    "\n",
    "        outcome_sta[status] += 1\n",
    "        outcome_sta_l[lex][status] += 1\n",
    "        outcome_lab[sense_label] += 1\n",
    "        outcome_lab_l[lex][sense_label] += 1\n",
    "        text = reptext('', L.d('phrase', c), num=True, txt=True)\n",
    "\n",
    "        of.write(fields_fmt.format(\n",
    "            book,\n",
    "            chapter,\n",
    "            verse,\n",
    "            sentence_n,\n",
    "            clause_n,\n",
    "            '\"'+lex+'\"',\n",
    "            stat_rep[status],\n",
    "            '\"-'+sense_label+'-\"',\n",
    "            '\"'+sense_txt+'\"',\n",
    "            action_stat,\n",
    "            '\"'+action_txt+'\"',\n",
    "            len(dos),\n",
    "            len(pdos),\n",
    "            len(sdos),\n",
    "            len(inds),\n",
    "            len(locs),\n",
    "            len(cpls),\n",
    "            '\"'+text+'\"',\n",
    "        ))\n",
    "        ofs.write(sfields_fmt.format(\n",
    "            version,\n",
    "            book,\n",
    "            chapter,\n",
    "            verse,\n",
    "            clause_atom_n,\n",
    "            'T',\n",
    "            '',\n",
    "            status,\n",
    "            note_keyword_base+(' val_{}'.format(stat_rep[status]) if status != '!' else ''),\n",
    "            '_{sl}_ [{nm}|{vb}] {st}'.format(\n",
    "                nm=F.number.v(L.u('phrase', v)),\n",
    "                vb=F.g_word_utf8.v(v),\n",
    "                st=sense_txt,\n",
    "                sl=sense_label,\n",
    "            ),\n",
    "        ))\n",
    "        nnotes[note_keyword_base] += 1\n",
    "        if action_txt != '':\n",
    "            ofs.write(sfields_fmt.format(\n",
    "                version,\n",
    "                book,\n",
    "                chapter,\n",
    "                verse,\n",
    "                clause_atom_n,\n",
    "                'T',\n",
    "                '',\n",
    "                action_stat,\n",
    "                note_keyword_base+(' val_{}'.format(stat_rep[status]) if status != '!' else ''),\n",
    "                action_txt,\n",
    "            ))\n",
    "            nnotes['action'] += 1\n",
    "            \n",
    "# generate notes for the promotion candidates\n",
    "            \n",
    "for c in cast_constituents:\n",
    "    if len(cast_constituents[c]) == 0: continue\n",
    "    w1 = L.d('word', c)[0]\n",
    "    book = F.book.v(L.u('book', w1))\n",
    "    chapter = F.chapter.v(L.u('chapter', w1))\n",
    "    verse = F.verse.v(L.u('verse', w1))\n",
    "    clause_atom_n = F.number.v(L.u('clause_atom', w1))\n",
    "    for p in cast_constituents[c]:\n",
    "        ps = reptext('', [p], num=True, gloss=True)\n",
    "        ofs.write(sfields_fmt.format(\n",
    "            version,\n",
    "            book,\n",
    "            chapter,\n",
    "            verse,\n",
    "            clause_atom_n,\n",
    "            'T',\n",
    "            '',\n",
    "            '?',\n",
    "            note_keyword_base+' val_cast',\n",
    "            'Cast {}: {} ==> {}'.format(ps, F.original.v(p), F.grammatical.v(p)),\n",
    "        ))\n",
    "        nnotes['cast'] += 1\n",
    "of.close()\n",
    "ofs.close()\n",
    "msg('Done')\n",
    "\n",
    "msg('Computed {} clauses with flowchart'.format(sum(outcome_sta.values())))\n",
    "msg('Added notes for cast constituents')\n",
    "ntot = 0\n",
    "for (lab, n) in sorted(nnotes.items(), key=lambda x: x[0]):\n",
    "    ntot += n\n",
    "    print('{:<10} notes: {}'.format(lab, n))\n",
    "print('{:<10} notes: {}'.format('Total', ntot))\n",
    "\n",
    "for lex in [''] + sorted(senses):\n",
    "    print('All lexemes with flowchart specification' if lex == '' else lex)\n",
    "    src_sta = outcome_sta if lex == '' else outcome_sta_l.get(lex, {})\n",
    "    src_lab = outcome_lab if lex == '' else outcome_lab_l.get(lex, {})\n",
    "    tot = 0\n",
    "    for (x, n) in sorted(src_sta.items()):\n",
    "        tot += n\n",
    "        print('     Status   {:<7}: {:>4} clauses'.format(status_rep[x], n))\n",
    "    print('     All status      : {:>4} clauses'.format(tot))\n",
    "    tot = 0\n",
    "    for (x, n) in sorted(src_lab.items()):\n",
    "        tot += n\n",
    "        print('     Sense    {:<7}: {:>4} clauses'.format(x, n))\n",
    "    print('     All senses     : {:>4} clauses'.format(tot))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
