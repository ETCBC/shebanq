{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://emdros.org\" target=\"_blank\"><img align=\"left\" src=\"files/images/Emdros-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
    "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAF2SHEBANQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constructs a relational database, *passage*, meant to support browsing of texts and highlighting of words.\n",
    "It contains the texts themselves, verse by verse, and it contains book and chapter information.\n",
    "The *passage* database also contains a lexicon, which is linked to the word occurrences.\n",
    "\n",
    "See the MySQL create statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.4\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.lib import Transcription\n",
    "from etcbc.preprocess import prepare\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "if 'version' not in locals(): version = '4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-06-29T05-30-49\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2015-10-01T10-35-26\n",
      "    23s LOGFILE=/Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/shebanq/__log__shebanq.txt\n",
      "    37s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  0.00s LOADING API with EXTRAs: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-06-29T05-30-49\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-10-01T10-35-26\n",
      "  0.01s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK shebanq AT 2015-10-02T14-54-30\n",
      "  0.00s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK shebanq AT 2015-10-02T14-54-30\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, 'lexicon', 'shebanq', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads minmonad maxmonad\n",
    "        book chapter verse\n",
    "        g_cons g_cons_utf8 g_word g_word_utf8 trailer_utf8\n",
    "        g_qere_utf8 qtrailer_utf8\n",
    "        language lex g_lex lex_utf8 sp pdp ls\n",
    "        vt vs gn nu ps st\n",
    "        nme pfm prs uvf vbe vbs\n",
    "        g_entry g_entry_heb gloss\n",
    "        phono phono_sep\n",
    "        function typ rela txt det\n",
    "        code tab\n",
    "        number\n",
    "    ''',''),\n",
    "    'prepare': prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data model\n",
    "\n",
    "The data model of the browsing database as as follows:\n",
    "\n",
    "There are tables ``book``, ``chapter``, ``verse``, ``word_verse``, ``lexicon``, ``clause_atom``.\n",
    "\n",
    "The tables ``book``, ``chapter``, ``verse``, ``clause_atom`` contain fields ``first_m``, ``last_m``, \n",
    "denoting the first and last monad number of that book, chapter, verse, clause_atom.\n",
    "\n",
    "A ``book``-record contains an identifier and the name of the book.\n",
    "\n",
    "A ``chapter``-record contains an identifier, the number of the chapter, and a foreign key to the record in the ``book`` table to which the chapter belongs.\n",
    "\n",
    "A ``verse``-record contains an identifier, the number of the verse, and a foreign key to the record in the ``chapter`` table to which the verse belongs. More over, it contains the text of the whole verse in two formats:\n",
    "\n",
    "In field ``text``: the plain unicode text string of the complete verse.\n",
    "\n",
    "In field ``xml``: a sequence of ``<w>`` elements, one for each word in the verse, containing the plain unicode text string of that word as element content.\n",
    "The monad number of that word is stored in an attribute value. \n",
    "The monad number is a globally unique sequence number of a word occurrence in the Hebrew Bible, going from 1 to precisely 426,555.\n",
    "There is also a lexical identifier stored in an attribute value.\n",
    "The lexical identifier points to the lexical entry that corresponds with the word.\n",
    "\n",
    "    <w m=\"2\" l=\"3\">רֵאשִׁ֖ית </w>\n",
    "\n",
    "As you see, the material between a word and the next word is appended to the first word. So, when you concatenate words, whitespace or other separators are needed.\n",
    "\n",
    "A ``word_verse``-record links a word to a verse. \n",
    "The monad number is in field ``anchor``, which is an integer, \n",
    "and the verse is specified in the field ``verse_id`` as foreign key.\n",
    "The field ``lexicon_id`` is a foreign key into the ``lexicon`` table.\n",
    "\n",
    "There is also a ``word`` table, meant to store all the information to generate a rich representation of the hebrew text,\n",
    "its syntactic structure, and some linguistic properties.\n",
    "See that notebook for a description and an example of the rich hebrew text representation.\n",
    "\n",
    "The rich data is added per word, but the data has a dependency on the verses the words are contained in.\n",
    "In general, information about sentences, clauses and phrases will be displayed on the first words of those objects,\n",
    "but if the object started in a previous verse, this information is repeated on the first word of that object in the\n",
    "current verse.\n",
    "This insures that the display of a verse is always self-contained.\n",
    "\n",
    "The ``word`` table has no field ``id``, its primary key is the field called ``word_number``. \n",
    "This fields contains the same monad number as is used in the field ``anchor`` of the table ``word_verse``.\n",
    "\n",
    "A ``clause_atom`` record contains an identifier, and the book to which it belongs, and its sequence number within \n",
    "that book.\n",
    "In SHEBANQ, manual annotations are linked to the clause atom, so we need this information to easily fetch comments to\n",
    "passages and to compose charts and csv files.\n",
    "\n",
    "## Lexicon\n",
    "\n",
    "A ``lexicon`` record contains the various lexical fields, such as identifiers, entry representations,\n",
    "additional lexical properties, and a gloss.\n",
    "\n",
    "We make sure that we translate lexical feature values into values used for the etcbc4.\n",
    "We need the following information per entry:\n",
    "\n",
    "* **id** a fresh id (see below), to be used in applications, unique over **entryid** and **lan**\n",
    "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
    "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
    "* **g_entryid** the Hebrew untransliteration of entryid, with the disambiguation marks unchanged, corresponds to the ``lex_utf8`` feature\n",
    "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
    "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
    "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
    "* **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
    "* **root** the root, obtained from ``rt``\n",
    "* **pos** the part of speech, obtained from ``sp``\n",
    "* **nametype** the type of named entity, obtained from ``sm``\n",
    "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
    "* **gloss** the gloss from ``gl``\n",
    "\n",
    "We construct the **id** from the ``lex`` feature as follows:\n",
    "\n",
    "* allocate a varchar(32)\n",
    "* the > is an alef, we translate it to A\n",
    "* the < is an ayin, we translate it to O\n",
    "* the / denotes a noun, we translate it to n\n",
    "* the [ denotes a verb, we translate it to v\n",
    "* the = is for disambiguation, we translate it to i\n",
    "* we prepend a language identifier, 1 for Hebrew, 2 for aramaic.\n",
    "\n",
    "This is sound, see the scheck in the extradata/lexicon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field transformation\n",
    "\n",
    "The lexical fields require a bit of attention.\n",
    "The specification in ``lex_fields`` below specifies the lexicon fields in the intended order.\n",
    "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
    "\n",
    "    (source, method, name, transformation table, data type, data size, data options, params)\n",
    "\n",
    "## source \n",
    "May contain one of the following:\n",
    "\n",
    "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
    "* None. \n",
    "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
    "* the name of an other field as shown in the **name** part of the specification. \n",
    "  In this case, **method** is a function, defined else where, that takes the value of that other field as argument. \n",
    "  The function is typically a transliteration, or a stripping action.\n",
    "\n",
    "## method\n",
    "May contain one of the following:\n",
    "\n",
    "* a code (string), indicating:\n",
    "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
    "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
    "    * ``id``: take the id for this entry as generated by the program\n",
    "    * ``lan``: take the language of this entry\n",
    "* a function taking one argument\n",
    "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
    "    * *to_heb*: transform the transliteration into real unicode Hebrew\n",
    "    * feature lookup functions such as ``F.lex.v``\n",
    "\n",
    "## name\n",
    "The name of the field in the to be constructed table ``lexicon`` in the database ``passage``.\n",
    "\n",
    "## data type\n",
    "The sql data type, such as ``int`` or ``varchar``, without the size and options.\n",
    "\n",
    "## data size\n",
    "The sql data size, which shows up between ``()`` after the data type\n",
    "\n",
    "## data options\n",
    "Any remaining type specification, such as `` character set utf8``.\n",
    "\n",
    "## params\n",
    "Params consists currently of 1 boolean, indicating whether the field is defined on all words of the object, or only on its first word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of ketiv/qere\n",
    "\n",
    "We make a list of the ketiv-qere items.\n",
    "It will be used by the *heb* and the *ktv* functions.\n",
    "\n",
    "*heb()* provides the surface text of a word.\n",
    "When the qere is different from the ketiv, the vocalized qere is chosen.\n",
    "It is the value of ``g_word_utf8`` except when a qere is present, \n",
    "in which case it is ``g_qere_utf8``, preceded by a masora circle.\n",
    "This is the sign for the user to use data view to inspect the *ketiv*.\n",
    "\n",
    "*ktv()* provides the surface text of a word, in case the ketiv is different from the qere.\n",
    "It is the value of ``g_word_utf8`` precisely when a qere is present, \n",
    "otherwise it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    34s Building qere index\n",
      "    34s Found 1892 qeres\n"
     ]
    }
   ],
   "source": [
    "qeres = {}\n",
    "masora = '֯'\n",
    "msg('Building qere index')\n",
    "for w in F.g_qere_utf8.s():\n",
    "    qeres[w] = (masora+F.g_qere_utf8.v(w), F.qtrailer_utf8.v(w))\n",
    "msg('Found {} qeres'.format(len(qeres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_id(entryid):\n",
    "    return entryid.rstrip('/[=')\n",
    "\n",
    "def to_heb(translit):\n",
    "    return Transcription.to_hebrew(Transcription.suffix_and_finales(translit)[0])\n",
    "\n",
    "def heb(n):\n",
    "    if n in qeres:\n",
    "        (trsep, wrdrep) = qeres[n]\n",
    "    else:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        wrdrep = F.g_word_utf8.v(n)\n",
    "    if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "    return wrdrep + trsep\n",
    "\n",
    "def ktv(n):\n",
    "    if n in qeres:\n",
    "        trsep = F.trailer_utf8.v(n)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        return F.g_word_utf8.v(n) + trsep    \n",
    "    return ''\n",
    "\n",
    "def lang(n):\n",
    "    return 'hbo' if F.language.v(n) == 'Hebrew' else 'arc'\n",
    "\n",
    "def df(f):\n",
    "    def g(n): \n",
    "        val = f(n)\n",
    "#        if val == None or val == \"None\" or val == \"none\" or val == \"NA\" or val == \"N/A\" or val == \"n/a\":\n",
    "        if val == None:\n",
    "            return '#null'\n",
    "        return val\n",
    "    return g\n",
    "\n",
    "lex_fields = (\n",
    "    (None, 'id', 'id', None, 'varchar', 32, ' primary key'),\n",
    "    (None, 'lan', 'lan', None, 'char', 4, ''),\n",
    "    (None, 'entry', 'entryid', None, 'varchar', 32, ''),\n",
    "    ('entryid', strip_id, 'entry', None, 'varchar', 32, ''),\n",
    "    ('entry', to_heb, 'entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('entryid', to_heb, 'entryid_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('vc', 'lex', 'g_entry', None, 'varchar', 32, ''),\n",
    "    ('g_entry', to_heb, 'g_entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
    "    ('rt', 'lex', 'root', None, 'varchar', 32, ''),\n",
    "    ('sp', 'lex', 'pos', None, 'varchar', 8, ''),\n",
    "    ('sm', 'lex', 'nametype', None, 'varchar', 16, ''),\n",
    "    ('ls', 'lex', 'subpos', None, 'varchar', 8, ''),\n",
    "    ('gl', 'lex', 'gloss', None, 'varchar', 32, ' character set utf8'),\n",
    ")\n",
    "word_fields = (\n",
    "    (F.monads.v, 'number', 'word', 'int', 4, ' primary key', False),\n",
    "    (heb, 'heb', 'word', 'varchar', 32, '', False),\n",
    "    (ktv, 'ktv', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_entry_heb.v, 'vlex', 'word', 'varchar', 32, '', False),\n",
    "    (F.lex_utf8.v, 'clex', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_word.v, 'tran', 'word', 'varchar', 32, '', False),\n",
    "    (F.phono.v, 'phono', 'word', 'varchar', 32, '', False),\n",
    "    (F.phono_sep.v, 'phono_sep', 'word', 'varchar', 8, '', False),\n",
    "    (F.lex.v, 'lex', 'word', 'varchar', 32, '', False),\n",
    "    (F.g_lex.v, 'glex', 'word', 'varchar', 32, '', False),\n",
    "    (F.gloss.v, 'gloss', 'word', 'varchar', 32, '', False),\n",
    "    (lang, 'lang', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.sp.v), 'pos', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.pdp.v), 'pdp', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.ls.v), 'subpos', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vt.v), 'tense', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vs.v), 'stem', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.gn.v), 'gender', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.nu.v), 'gnumber', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.ps.v), 'person', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.st.v), 'state', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.nme.v), 'nme', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.pfm.v), 'pfm', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.prs.v), 'prs', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.uvf.v), 'uvf', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vbe.v), 'vbe', 'word', 'varchar', 8, '', False),\n",
    "    (df(F.vbs.v), 'vbs', 'word', 'varchar', 8, '', False),\n",
    "    (None, 'border', 'subphrase', 'varchar', 16, '', False),\n",
    "    ('id', 'number', 'subphrase', 'varchar', 32, '', False),\n",
    "    (df(F.rela.v), 'rela', 'subphrase', 'varchar', 8, '', True),\n",
    "    (None, 'border', 'phrase', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'phrase_atom', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'phrase_atom', 'varchar', 8, '', True),\n",
    "    (F.number.v, 'number', 'phrase', 'int', 4, '', False),\n",
    "    (df(F.function.v), 'function', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.rela.v), 'rela', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.typ.v), 'typ', 'phrase', 'varchar', 8, '', True),\n",
    "    (df(F.det.v), 'det', 'phrase', 'varchar', 8, '', True),\n",
    "    (None, 'border', 'clause', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'clause_atom', 'int', 4, '', False),\n",
    "    (df(F.code.v), 'code', 'clause_atom', 'varchar', 8, '', True),\n",
    "    (df(F.tab.v), 'tab', 'clause_atom', 'int', 4, '', False),\n",
    "    (F.number.v, 'number', 'clause', 'int', 4, '', False),\n",
    "    (df(F.rela.v), 'rela', 'clause', 'varchar', 8, '', True),\n",
    "    (df(F.typ.v), 'typ', 'clause', 'varchar', 8, '', True),\n",
    "    (df(F.txt.v), 'txt', 'clause', 'varchar', 8, '', False),\n",
    "    (None, 'border', 'sentence', 'varchar', 8, '', False),\n",
    "    (F.number.v, 'number', 'sentence_atom', 'int', 4, '', False),\n",
    "    (F.number.v, 'number', 'sentence', 'int', 4, '', False),\n",
    ")\n",
    "first_only = dict(('{}_{}'.format(f[2], f[1]), f[6]) for f in word_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity\n",
    "The texts and xml representations of verses are stored in ``varchar`` fields.\n",
    "We have to make sure that the values fit within the declared sizes of these fields.\n",
    "The code measures the maximum lengths of these fields, and it turns out that the text is maximally 434 chars and the xml 2186 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drop database if exists shebanq_passage4b;\n",
      "\n",
      "create database shebanq_passage4b character set utf8;\n",
      "\n",
      "use shebanq_passage4b;\n",
      "\n",
      "create table book(\n",
      "    id      int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    name varchar(32),\n",
      "    index(name)\n",
      ");\n",
      "\n",
      "create table chapter(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    book_id int(4),\n",
      "    chapter_num int(4),\n",
      "    foreign key (book_id) references book(id),\n",
      "    index(chapter_num)\n",
      ");\n",
      "\n",
      "create table verse(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    chapter_id int(4),\n",
      "    verse_num int(4),\n",
      "    text varchar(1024) character set utf8,\n",
      "    xml varchar(4096) character set utf8,\n",
      "    foreign key (chapter_id) references chapter(id)\n",
      ");\n",
      "\n",
      "create table clause_atom(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    ca_num int(4),    \n",
      "    book_id int(4),\n",
      "    text varchar(512) character set utf8,\n",
      "    foreign key (book_id) references book(id),\n",
      "    index(ca_num)\n",
      ");\n",
      "\n",
      "create table word(\n",
      "    word_number int(4) primary key, \n",
      "    word_heb varchar(32), \n",
      "    word_ktv varchar(32), \n",
      "    word_vlex varchar(32), \n",
      "    word_clex varchar(32), \n",
      "    word_tran varchar(32), \n",
      "    word_phono varchar(32), \n",
      "    word_phono_sep varchar(8), \n",
      "    word_lex varchar(32), \n",
      "    word_glex varchar(32), \n",
      "    word_gloss varchar(32), \n",
      "    word_lang varchar(8), \n",
      "    word_pos varchar(8), \n",
      "    word_pdp varchar(8), \n",
      "    word_subpos varchar(8), \n",
      "    word_tense varchar(8), \n",
      "    word_stem varchar(8), \n",
      "    word_gender varchar(8), \n",
      "    word_gnumber varchar(8), \n",
      "    word_person varchar(8), \n",
      "    word_state varchar(8), \n",
      "    word_nme varchar(8), \n",
      "    word_pfm varchar(8), \n",
      "    word_prs varchar(8), \n",
      "    word_uvf varchar(8), \n",
      "    word_vbe varchar(8), \n",
      "    word_vbs varchar(8), \n",
      "    subphrase_border varchar(16), \n",
      "    subphrase_number varchar(32), \n",
      "    subphrase_rela varchar(8), \n",
      "    phrase_border varchar(8), \n",
      "    phrase_atom_number int(4), \n",
      "    phrase_atom_rela varchar(8), \n",
      "    phrase_number int(4), \n",
      "    phrase_function varchar(8), \n",
      "    phrase_rela varchar(8), \n",
      "    phrase_typ varchar(8), \n",
      "    phrase_det varchar(8), \n",
      "    clause_border varchar(8), \n",
      "    clause_atom_number int(4), \n",
      "    clause_atom_code varchar(8), \n",
      "    clause_atom_tab int(4), \n",
      "    clause_number int(4), \n",
      "    clause_rela varchar(8), \n",
      "    clause_typ varchar(8), \n",
      "    clause_txt varchar(8), \n",
      "    sentence_border varchar(8), \n",
      "    sentence_atom_number int(4), \n",
      "    sentence_number int(4)\n",
      ");\n",
      "\n",
      "create table lexicon(\n",
      "    id varchar(32) primary key,\n",
      "    lan char(4),\n",
      "    entryid varchar(32),\n",
      "    entry varchar(32),\n",
      "    entry_heb varchar(32) character set utf8,\n",
      "    entryid_heb varchar(32) character set utf8,\n",
      "    g_entry varchar(32),\n",
      "    g_entry_heb varchar(32) character set utf8,\n",
      "    root varchar(32),\n",
      "    pos varchar(8),\n",
      "    nametype varchar(16),\n",
      "    subpos varchar(8),\n",
      "    gloss varchar(32) character set utf8    \n",
      ") collate utf8_bin;\n",
      "\n",
      "create table word_verse(\n",
      "    anchor int(4) unique,\n",
      "    verse_id int(4),\n",
      "    lexicon_id varchar(32),\n",
      "    foreign key (anchor) references word(word_number),\n",
      "    foreign key (verse_id) references verse(id),\n",
      "    foreign key (lexicon_id) references lexicon(id)\n",
      ") collate utf8_bin;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field_limits = {\n",
    "    'book': {\n",
    "        'name': 32,\n",
    "    },\n",
    "    'verse': {\n",
    "        'text': 1024,\n",
    "        'xml': 4096,\n",
    "    },\n",
    "    'clause_atom': {\n",
    "        'text': 512,\n",
    "    },\n",
    "    'lexicon': {},\n",
    "}\n",
    "for f in lex_fields:\n",
    "    if f[4].endswith('char'):\n",
    "        field_limits['lexicon'][f[2]] = f[5]\n",
    "\n",
    "config = {\n",
    "    'db': 'shebanq_passage'+version,\n",
    "}\n",
    "for tb in field_limits:\n",
    "    for fl in field_limits[tb]: config['{}_{}'.format(tb, fl)] = field_limits[tb][fl]\n",
    "\n",
    "text_create_sql = '''\n",
    "drop database if exists {db};\n",
    "\n",
    "create database {db} character set utf8;\n",
    "\n",
    "use {db};\n",
    "\n",
    "create table book(\n",
    "    id      int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    name varchar({book_name}),\n",
    "    index(name)\n",
    ");\n",
    "\n",
    "create table chapter(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    book_id int(4),\n",
    "    chapter_num int(4),\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(chapter_num)\n",
    ");\n",
    "\n",
    "create table verse(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    chapter_id int(4),\n",
    "    verse_num int(4),\n",
    "    text varchar({verse_text}) character set utf8,\n",
    "    xml varchar({verse_xml}) character set utf8,\n",
    "    foreign key (chapter_id) references chapter(id)\n",
    ");\n",
    "\n",
    "create table clause_atom(\n",
    "    id int(4) primary key,\n",
    "    first_m int(4),\n",
    "    last_m int(4),\n",
    "    ca_num int(4),    \n",
    "    book_id int(4),\n",
    "    text varchar({clause_atom_text}) character set utf8,\n",
    "    foreign key (book_id) references book(id),\n",
    "    index(ca_num)\n",
    ");\n",
    "\n",
    "create table word(\n",
    "    {{wordfields}}\n",
    ");\n",
    "\n",
    "create table lexicon(\n",
    "    {{lexfields}}    \n",
    ") collate utf8_bin;\n",
    "\n",
    "create table word_verse(\n",
    "    anchor int(4) unique,\n",
    "    verse_id int(4),\n",
    "    lexicon_id varchar(32),\n",
    "    foreign key (anchor) references word(word_number),\n",
    "    foreign key (verse_id) references verse(id),\n",
    "    foreign key (lexicon_id) references lexicon(id)\n",
    ") collate utf8_bin;\n",
    "\n",
    "'''.format(**config).format(\n",
    "        lexfields = ',\\n    '.join('{} {}({}){}'.format(\n",
    "            f[2], f[4], f[5], f[6],\n",
    "        ) for f in lex_fields),\n",
    "        wordfields = ', \\n    '.join('{}_{} {}({}){}'.format(\n",
    "            f[2], f[1], f[3], f[4], f[5],\n",
    "    ) for f in word_fields),\n",
    ")\n",
    "print(text_create_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lexicon arc: there were 0 errors\n",
      "feature without value for lexical entry MCXH==/ in line 4779: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon arc has   708 entries\n",
      "Lexicon hbo has  8518 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lexicon hbo: there was 1 error\n"
     ]
    }
   ],
   "source": [
    "langs = {'hbo', 'arc'}\n",
    "lex_base = dict((lan, '{}/{}/{}.{}{}'.format(API['data_dir'], 'lexicon', lan, source, version)) for lan in langs)\n",
    "lang_map = {\n",
    "    'Hebrew': 'hbo',\n",
    "    'Aramaic': 'arc',\n",
    "}\n",
    "\n",
    "def read_lex(lan):\n",
    "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
    "\n",
    "    lex_items = {}\n",
    "    ln = 0\n",
    "    e = 0\n",
    "    for line in lex_infile:\n",
    "        ln += 1\n",
    "        line = line.split('#')[0]\n",
    "        line = line.rstrip()\n",
    "        if line == '': continue\n",
    "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
    "        entry = entry.strip('\"')\n",
    "        if entry in lex_items:\n",
    "            sys.stderr.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
    "            e += 1\n",
    "            continue\n",
    "        if featurestr.startswith(':') and featurestr.endswith(':'):\n",
    "            featurestr = featurestr.strip(':')\n",
    "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
    "        featurelst = featurestr.split(':')\n",
    "        features = {}\n",
    "        for feature in featurelst:\n",
    "            comps = feature.split('=', maxsplit=1)\n",
    "            if len(comps) == 1:\n",
    "                if feature.strip().isnumeric():\n",
    "                    comps = ('_n', feature.strip())\n",
    "                else:\n",
    "                    sys.stderr.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
    "                    e += 1\n",
    "                    continue\n",
    "            (key, value) = comps\n",
    "            value = value.replace(chr(254), ':')\n",
    "            if key in features:\n",
    "                sys.stderr.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
    "                e += 1\n",
    "                continue\n",
    "            features[key] = value\n",
    "        if 'sp' in features and features['sp'] == 'verb':\n",
    "            if 'gl' in features:\n",
    "                gloss = features['gl']\n",
    "                if gloss.startswith('to '):\n",
    "                    features['gl'] = gloss[3:]\n",
    "        lex_items[entry] = features\n",
    "        \n",
    "    lex_infile.close()\n",
    "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error') + '\\n'\n",
    "    sys.stderr.write(msgstr)\n",
    "    return lex_items\n",
    "\n",
    "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
    "for lan in sorted(lex_entries):\n",
    "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon result\n",
    "The result is also stored in a tab separated file, which can be downloaded from my\n",
    "[SURFdrive](https://surfdrive.surf.nl/files/public.php?service=files&t=f910f1e088d1dfc9fc526e408ab07c45)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table filling\n",
    "\n",
    "We compose all the records for all the tables.\n",
    "\n",
    "We also generate a file that can act as the basis of an extra annotation file with lexical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 01s Fill the tables ... \n",
      " 1m 14s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK    book           name           : max size =      13 of    32\n",
      "OK    clause_atom    text           : max size =     261 of   512\n",
      "OK    lexicon        entry          : max size =      14 of    32\n",
      "OK    lexicon        entry_heb      : max size =      14 of    32\n",
      "OK    lexicon        entryid        : max size =      15 of    32\n",
      "OK    lexicon        entryid_heb    : max size =      15 of    32\n",
      "OK    lexicon        g_entry        : max size =      24 of    32\n",
      "OK    lexicon        g_entry_heb    : max size =      23 of    32\n",
      "OK    lexicon        gloss          : max size =      27 of    32\n",
      "OK    lexicon        id             : max size =      16 of    32\n",
      "OK    lexicon        lan            : max size =       3 of     4\n",
      "OK    lexicon        nametype       : max size =      14 of    16\n",
      "OK    lexicon        pos            : max size =       4 of     8\n",
      "OK    lexicon        root           : max size =      11 of    32\n",
      "OK    lexicon        subpos         : max size =       4 of     8\n",
      "OK    verse          text           : max size =     434 of  1024\n",
      "OK    verse          xml            : max size =    2858 of  4096\n",
      "All lexemes have been found in the lexicon\n"
     ]
    }
   ],
   "source": [
    "msg(\"Fill the tables ... \")\n",
    "cur_id = {\n",
    "    'book': -1,\n",
    "    'chapter': - 1,\n",
    "    'verse': -1,\n",
    "    'clause_atom': -1\n",
    "}\n",
    "\n",
    "def s_esc(sql): return sql.replace(\"'\", \"''\").replace('\\\\','\\\\\\\\').replace('\\n','\\\\n')\n",
    "\n",
    "cur_verse_node = None\n",
    "cur_verse_info = []\n",
    "cur_verse_first_m = None\n",
    "cur_verse_last_m = None\n",
    "cur_lex_values = {}\n",
    "\n",
    "lex_index = {}\n",
    "lex_not_found = collections.defaultdict(lambda: collections.Counter())\n",
    "tables = collections.defaultdict(lambda: [])\n",
    "field_sizes = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "Fotypev = F.otype.v\n",
    "Fmonadsv = F.monads.v\n",
    "Fmin = F.minmonad.v\n",
    "Fmax = F.maxmonad.v\n",
    "Ftextv = F.g_word_utf8.v\n",
    "Foccv = F.g_cons.v\n",
    "Flexv = F.lex.v\n",
    "Flanguagev = F.language.v\n",
    "Ftrailerv = F.trailer_utf8.v\n",
    "Fnumberv = F.number.v\n",
    "\n",
    "dqf = outfile('etcbc4-lexicon.tsv')\n",
    "dqf.write('{}\\n'.format('\\t'.join(x[2] for x in lex_fields)))\n",
    "\n",
    "def compute_fields(lan, entry, lexfeats):\n",
    "    cur_lex_values.clear()\n",
    "    return tuple(compute_field(lan, entry, lexfeats, f) for f in lex_fields)\n",
    "\n",
    "def compute_field(lan, entry, lexfeats, f):\n",
    "    (source, method, name, transform, datatype, datasize, dataoption) = f\n",
    "    val = None\n",
    "    if method == 'lan': val = lan\n",
    "    elif method == 'entry': val = entry\n",
    "    elif method == 'id':\n",
    "        val = '{}{}'.format(\n",
    "            '1' if lan == 'hbo' else '2',\n",
    "            entry.\n",
    "                replace('>','A').\n",
    "                replace('<','O').\n",
    "                replace('[','v').\n",
    "                replace('/','n').\n",
    "                replace('=','i'),\n",
    "        )\n",
    "        lex_index[(lan, entry)] = val\n",
    "    elif method =='lex':\n",
    "        val = s_esc(lexfeats.get(source, ''))\n",
    "        if transform != None and val in transform: val = transform[val]\n",
    "    else: val = method(cur_lex_values[source])\n",
    "    cur_lex_values[name] = val\n",
    "    if name in field_limits['lexicon']:\n",
    "        field_sizes['lexicon'][name] = max(len(val), field_sizes['lexicon'][name])\n",
    "    return val\n",
    "\n",
    "for lan in sorted(lex_entries):\n",
    "    for entry in sorted(lex_entries[lan]):\n",
    "        format_str = '({})'.format(','.join('{}' if f[4] == 'int' else \"'{}'\" for f in lex_fields))\n",
    "        entry_info = compute_fields(lan, entry, lex_entries[lan][entry])\n",
    "        dqf.write('{}\\n'.format('\\t'.join(str(x) for x in entry_info)))\n",
    "        tables['lexicon'].append(format_str.format(\n",
    "            *entry_info\n",
    "        ))\n",
    "dqf.close()\n",
    "\n",
    "def do_verse(node):\n",
    "    global cur_verse_node, cur_verse_info, max_len_text, max_len_xml\n",
    "    if cur_verse_node != None:\n",
    "        this_text = ''.join('{}{}'.format(x[0], x[1]) for x in cur_verse_info)\n",
    "        this_xml = ''.join(\n",
    "            '''<w m=\"{}\" t=\"{}\" l=\"{}\">{}</w>'''.format(\n",
    "                x[2], x[1].replace('\\n', '&#xa;'), x[4], x[0]\n",
    "            ) for x in cur_verse_info)\n",
    "        field_sizes['verse']['text'] = max((len(this_text), field_sizes['verse']['text']))\n",
    "        field_sizes['verse']['xml'] = max((len(this_xml), field_sizes['verse']['xml']))\n",
    "        tables['verse'].append(\"({},{},{},{},{},'{}','{}')\".format(\n",
    "            cur_id['verse'], \n",
    "            cur_verse_first_m, \n",
    "            cur_verse_last_m, \n",
    "            cur_id['chapter'], F.verse.v(cur_verse_node), s_esc(this_text), s_esc(this_xml),\n",
    "        ))\n",
    "        for x in cur_verse_info:\n",
    "            tables['word_verse'].append(\"({}, {}, '{}')\".format(\n",
    "                x[2], x[3], x[4]\n",
    "            ))\n",
    "        cur_verse_info = []\n",
    "    cur_verse_node = node    \n",
    "\n",
    "for node in NN():\n",
    "    otype = Fotypev(node)\n",
    "    if otype == 'word':\n",
    "        if node in qeres:\n",
    "            (text, trailer) = qeres[node]\n",
    "        else:\n",
    "            text = Ftextv(node)\n",
    "            trailer = Ftrailerv(node)\n",
    "        if trailer.endswith('ס') or trailer.endswith('פ'): trailer += ' '\n",
    "        lex = Flexv(node)\n",
    "        lang = Flanguagev(node)\n",
    "        lid = lex_index.get((lang_map[lang], lex), None)\n",
    "        if lid == None:\n",
    "            lex_not_found[(lang_map[lang], lex)][Foccv(node)] += 1\n",
    "        cur_verse_info.append((\n",
    "            text,\n",
    "            trailer,\n",
    "            Fmonadsv(node), \n",
    "            cur_id['verse'],\n",
    "            lid,\n",
    "        ))\n",
    "    elif otype == 'verse':\n",
    "        do_verse(node)\n",
    "        cur_id['verse'] += 1\n",
    "        cur_verse_first_m = Fmin(node)\n",
    "        cur_verse_last_m = Fmax(node)\n",
    "    elif otype == 'chapter':\n",
    "        do_verse(None)\n",
    "        cur_id['chapter'] += 1\n",
    "        tables['chapter'].append(\"({},{},{},{},{})\".format(\n",
    "            cur_id['chapter'], Fmin(node), Fmax(node), cur_id['book'], F.chapter.v(node),\n",
    "        ))\n",
    "    elif otype == 'book':\n",
    "        do_verse(None)\n",
    "        cur_id['book'] += 1\n",
    "        name = F.book.v(node)\n",
    "        field_sizes['book']['name'] = max((len(name), field_sizes['book']['name']))\n",
    "        tables['book'].append(\"({},{},{},'{}')\".format(\n",
    "            cur_id['book'], Fmin(node), Fmax(node), s_esc(name),\n",
    "        ))\n",
    "    elif otype == 'clause_atom':\n",
    "        cur_id['clause_atom'] += 1\n",
    "        ca_num = Fnumberv(node)\n",
    "        wordtexts = []\n",
    "        for w in L.d('word', node):\n",
    "            trsep = Ftrailerv(w)\n",
    "            if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "            wordtexts.append(F.g_word_utf8.v(w) +trsep)\n",
    "        text = ''.join(wordtexts)\n",
    "        field_sizes['clause_atom']['text'] = max((len(text), field_sizes['clause_atom']['text']))\n",
    "        tables['clause_atom'].append(\"({},{},{},{},{},'{}')\".format(\n",
    "            cur_id['clause_atom'], Fmin(node), Fmax(node), ca_num, cur_id['book'], s_esc(text),\n",
    "        ))\n",
    "do_verse(None)\n",
    "\n",
    "for tb in sorted(field_limits):\n",
    "    for fl in sorted(field_limits[tb]):\n",
    "        limit = field_limits[tb][fl]\n",
    "        actual = field_sizes[tb][fl]\n",
    "        exceeded = actual > limit\n",
    "        outp = sys.stderr if exceeded else sys.stdout\n",
    "        outp.write('{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}\\n'.format(\n",
    "            'ERROR' if exceeded else 'OK',\n",
    "            tb, fl, actual, limit,\n",
    "        ))\n",
    "\n",
    "msg(\"Done\")\n",
    "if len(lex_not_found):\n",
    "    sys.stderr.write('Text lexemes not found in lexicon: {}x\\n'.format(len(lex_not_found)))\n",
    "    for l in sorted(lex_not_found):\n",
    "        sys.stderr.write('{} {}\\n'.format(*l))\n",
    "        for (o, n) in sorted(lex_not_found[l].items(), key=lambda x: (-x[1], x[0])):\n",
    "            sys.stderr.write('\\t{}: {}x\\n'.format(o, n))\n",
    "else:\n",
    "    print('All lexemes have been found in the lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2OBDn','arc','<BD/','<BD','עבד','עבד/','<:AB;D','עֲבֵד','','subs','','','servant')\n",
      "('2OBDv','arc','<BD[','<BD','עבד','עבד[','<:ABAD','עֲבַד','','verb','','','do')\n",
      "('2OBD_NGWn','arc','<BD_NGW/','<BD_NGW','עבד נגו','עבד נגו/','<:AB;D_N:GOW','עֲבֵד נְגֹו','','nmpr','pers','','Abed-Nego')\n",
      "('2OBJDHn','arc','<BJDH/','<BJDH','עבידה','עבידה/','<:ABIJDAH','עֲבִידַה','','subs','','','work')\n",
      "('2OBRn','arc','<BR/','<BR','עבר','עבר/','<:ABAR','עֲבַר','','subs','','ppre','opposite bank')\n",
      "('2OCTv','arc','<CT[','<CT','עשׁת','עשׁת[','<:ACIT','עֲשִׁת','','verb','','','intend')\n",
      "('2OD','arc','<D','<D','עד','עד','<AD','עַד','','prep','','','until')\n",
      "('2ODHv','arc','<DH[','<DH','עדה','עדה[','<:AD@H','עֲדָה','','verb','','','go')\n",
      "('2ODNn','arc','<DN/','<DN','עדן','עדן/','<ID.@N','עִדָּן','','subs','','','time')\n",
      "('2ODWAn','arc','<DW>/','<DW>','עדוא','עדוא/','<ID.OW>','עִדֹּוא','','nmpr','pers','','Iddo')\n",
      "(0,1,11,1,0,'בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\\n')\n",
      "(1,12,18,2,0,'וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ ')\n",
      "(2,19,23,3,0,'וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְהֹ֑ום ')\n",
      "(3,24,31,4,0,'וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃\\n')\n",
      "(4,32,34,5,0,'וַיֹּ֥אמֶר אֱלֹהִ֖ים ')\n",
      "(5,35,36,6,0,'יְהִ֣י אֹ֑ור ')\n",
      "(6,37,39,7,0,'וַֽיְהִי־אֹֽור׃\\n')\n",
      "(7,40,45,8,0,'וַיַּ֧רְא אֱלֹהִ֛ים אֶת־הָאֹ֖ור ')\n",
      "(8,46,47,9,0,'כִּי־טֹ֑וב ')\n",
      "(9,48,57,10,0,'וַיַּבְדֵּ֣ל אֱלֹהִ֔ים בֵּ֥ין הָאֹ֖ור וּבֵ֥ין הַחֹֽשֶׁךְ׃\\n')\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(tables['lexicon'][0:10]))\n",
    "print('\\n'.join(tables['clause_atom'][0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra word data\n",
    "\n",
    "Now we fetch the data needed for representing rich hebrew text.\n",
    "\n",
    "## Passage index\n",
    "When we have found our objects, we want to indicate where they occur in the bible. In order to specify the passage of a node, we have to now in what verse a node occurs. In the next code cell we create a mapping from nodes of type sentence, clause, etc to nodes of type verse. From a verse node we can read off the passage information.\n",
    "\n",
    "Conversely, we also construct an index from verses to nodes: given a verse, we make a list of all nodes belonging to that verse, in the canonical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_types = {\n",
    "    'sentence', 'sentence_atom', \n",
    "    'clause', 'clause_atom', \n",
    "    'phrase', 'phrase_atom', \n",
    "    'subphrase',\n",
    "}\n",
    "\n",
    "def get_set(monads):\n",
    "    monad_set = set()\n",
    "    for rn in monads.split(','):\n",
    "        bnds = rn.split('-', 1)\n",
    "        if len(bnds) == 1:\n",
    "            monad_set.add(int(bnds[0]))\n",
    "        else: \n",
    "            monad_set |= set(range(int(bnds[0]), int(bnds[1]) + 1))\n",
    "    return frozenset(monad_set)\n",
    "\n",
    "def ranges(monadset):\n",
    "    result = []\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "    for i in sorted(monadset):\n",
    "        if cur_start == None:\n",
    "            cur_start = i\n",
    "            cur_end = i\n",
    "        else:\n",
    "            if i == cur_end + 1:\n",
    "                cur_end += 1\n",
    "            else:\n",
    "                result.append((cur_start, cur_end))\n",
    "                cur_start = i\n",
    "                cur_end = i\n",
    "    if cur_start != None:\n",
    "        result.append((cur_start, cur_end))\n",
    "    return result\n",
    "\n",
    "def get_objects(vn):\n",
    "    objects = set()\n",
    "    for wn in L.d('word', vn):\n",
    "        objects.add(wn)\n",
    "        for tt in target_types:\n",
    "            on = L.u(tt, wn)\n",
    "            if on != None: objects.add(on)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the word info table with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2m 52s Generating word info data ...\n",
      " 2m 53s \tGenesis\n",
      " 3m 11s \tExodus\n",
      " 3m 26s \tLeviticus\n",
      " 3m 37s \tNumeri\n",
      " 3m 53s \tDeuteronomium\n",
      " 4m 06s \tJosua\n",
      " 4m 16s \tJudices\n",
      " 4m 26s \tSamuel_I\n",
      " 4m 41s \tSamuel_II\n",
      " 4m 52s \tReges_I\n",
      " 5m 06s \tReges_II\n",
      " 5m 19s \tJesaia\n",
      " 5m 36s \tJeremia\n",
      " 6m 02s \tEzechiel\n",
      " 6m 23s \tHosea\n",
      " 6m 25s \tJoel\n",
      " 6m 26s \tAmos\n",
      " 6m 28s \tObadia\n",
      " 6m 29s \tJona\n",
      " 6m 29s \tMicha\n",
      " 6m 31s \tNahum\n",
      " 6m 31s \tHabakuk\n",
      " 6m 32s \tZephania\n",
      " 6m 33s \tHaggai\n",
      " 6m 33s \tSacharia\n",
      " 6m 37s \tMaleachi\n",
      " 6m 37s \tPsalmi\n",
      " 6m 55s \tIob\n",
      " 7m 05s \tProverbia\n",
      " 7m 14s \tRuth\n",
      " 7m 15s \tCanticum\n",
      " 7m 17s \tEcclesiastes\n",
      " 7m 20s \tThreni\n",
      " 7m 22s \tEsther\n",
      " 7m 25s \tDaniel\n",
      " 7m 31s \tEsra\n",
      " 7m 37s \tNehemia\n",
      " 7m 44s \tChronica_I\n",
      " 7m 53s \tChronica_II\n",
      " 8m 06s Done\n"
     ]
    }
   ],
   "source": [
    "msg(\"Generating word info data ...\")\n",
    "wordf = outfile('word_data.tsv')\n",
    "#wordrf = outfile('word_r_data.tsv')\n",
    "plainf = outfile('verse_plain.txt')\n",
    "wordf.write('{}\\n'.format('\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
    "#wordrf.write('{}\\t{}\\t{}\\t{}\\n'.format('book', 'chapter', 'verse', '\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
    "tables['word'] = []\n",
    "\n",
    "if 'word' in field_sizes: del field_sizes['word']\n",
    "\n",
    "def do_verse_info(verse):\n",
    "    vlabel = '{} {}:{}'.format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse))\n",
    "    wordf.write('!{}\\n'.format(vlabel))\n",
    "#    monads = verse_monads[verse]\n",
    "    monads = {int(F.monads.v(w)) for w in L.d('word', verse)}\n",
    "    \n",
    "    (verse_startm, verse_endm) = (min(monads), max(monads))\n",
    "#    objects = verse_node[verse]\n",
    "    objects = get_objects(verse)\n",
    "    words = [dict() for i in range(verse_startm, verse_endm + 1)]\n",
    "    for w in words:\n",
    "        for (otype, do_border) in (\n",
    "            ('sentence', True), \n",
    "            ('sentence_atom', False), \n",
    "            ('clause', True), \n",
    "            ('clause_atom', False), \n",
    "            ('phrase', True),\n",
    "            ('phrase_atom', False),\n",
    "            ('subphrase', True),\n",
    "            ('word', False),\n",
    "        ):\n",
    "            w['{}_{}'.format(otype, 'number')] = list()\n",
    "            if do_border:\n",
    "                w['{}_{}'.format(otype, 'border')] = set()\n",
    "    nwords = len(words)\n",
    "    subphrase_counter = 0\n",
    "    word_nodes = []\n",
    "    for n in objects:\n",
    "        otype = F.otype.v(n)\n",
    "        if otype == 'word': word_nodes.append(n)\n",
    "        number_prop = '{}_{}'.format(otype, 'number')\n",
    "        if otype != 'word' and not otype.endswith('_atom'):\n",
    "            border_prop = '{}_{}'.format(otype, 'border')\n",
    "        else:\n",
    "            border_prop = None\n",
    "\n",
    "        if otype == 'subphrase': subphrase_counter += 1\n",
    "        elif otype in {'phrase', 'clause', 'sentence'}: subphrase_counter = 0\n",
    "# Here was a bug: I put the subphrase_counter to 0 upon encountering anything else than a subphrase or a word.\n",
    "# I had overlooked the half_verse, which can cut through a phrase\n",
    "        this_info = {}\n",
    "        this_number = None\n",
    "        for f in word_fields:\n",
    "            (method, name, typ) = (f[0], '{}_{}'.format(f[2], f[1]), f[3])\n",
    "            if otype != f[2] or method == None: continue\n",
    "            if method == 'id':\n",
    "                value = subphrase_counter\n",
    "            else:\n",
    "                value = method(n)\n",
    "                if typ == 'int': value = int(value)\n",
    "            if name == number_prop:\n",
    "                this_number = value\n",
    "            else:\n",
    "                this_info[name] = value\n",
    "        if otype == 'word':\n",
    "            target = words[this_number - verse_startm]\n",
    "            target.update(this_info)\n",
    "            target[number_prop].append(this_number)            \n",
    "        else:\n",
    "            these_ranges = ranges(get_set(F.monads.v(n)))\n",
    "            nranges = len(these_ranges) - 1\n",
    "            for (e,r) in enumerate(these_ranges):\n",
    "                is_first = e == 0\n",
    "                is_last = e == nranges\n",
    "                right_border = 'rr' if is_first else 'r'\n",
    "                left_border = 'll' if is_last else 'l'\n",
    "                first_word = -1 if r[0] < verse_startm else nwords if r[0] > verse_endm else r[0] - verse_startm\n",
    "                last_word = -1 if r[1] < verse_startm else nwords if r[1] > verse_endm else r[1] - verse_startm\n",
    "                my_first_word = max(first_word, 0)\n",
    "                my_last_word = min(last_word, nwords - 1)\n",
    "                for i in range(my_first_word, my_last_word + 1):\n",
    "                    target = words[i]\n",
    "                    if not first_only[number_prop] or i == my_first_word:\n",
    "                        target[number_prop].append(this_number)\n",
    "                    for f in this_info:\n",
    "                        if not first_only[name] or i == my_first_word:\n",
    "                            words[i][f] = this_info[f]\n",
    "                    if otype == 'subphrase':\n",
    "                        if border_prop != None: words[i][border_prop].add('sy')\n",
    "                if 0 <= first_word < nwords:\n",
    "                    if border_prop != None: words[first_word][border_prop].add(right_border)\n",
    "                if 0 <= last_word < nwords:\n",
    "                    if border_prop != None: words[last_word][border_prop].add(left_border)\n",
    "    wordtext = []\n",
    "    for w in word_nodes:\n",
    "        trsep = Ftrailerv(w)\n",
    "        if trsep.endswith('ס') or trsep.endswith('פ'): trsep += ' '\n",
    "        wordtext.append(F.g_word_utf8.v(w) +trsep)\n",
    "    plainf.write(\"{}\\t{}\\n\".format(\n",
    "        vlabel, \n",
    "        ''.join(wt for wt in wordtext).replace('\\n', '\\\\n'),\n",
    "    ))\n",
    "    for w in words:\n",
    "        row = []\n",
    "        rrow = []\n",
    "        for f in word_fields:\n",
    "            typ = f[3]\n",
    "            name = '{}_{}'.format(f[2], f[1])\n",
    "            value = w.get(name, 'NULL' if typ == 'int' else '')\n",
    "            if f[1] == 'border':\n",
    "                value = ' '.join(value)\n",
    "            elif f[1] == 'number':\n",
    "                value = ' '.join(str(v) for v in value)\n",
    "            rrow.append(str(value).replace('\\n', '\\\\n').replace('\\t', '\\\\t'))\n",
    "            if typ == 'int':\n",
    "                value = str(value)\n",
    "            else:\n",
    "                if typ.endswith('char'):\n",
    "                    lvalue = len(value)\n",
    "                    curlen = field_sizes['word'][name]\n",
    "                    if lvalue > curlen: field_sizes['word'][name] = lvalue\n",
    "                value = \"'{}'\".format(s_esc(value))\n",
    "            row.append(value)\n",
    "        tables['word'].append('({})'.format(','.join(row)))\n",
    "        wordf.write(\"{}\\n\".format('\\t'.join(rrow)))\n",
    "        #wordrf.write(\"{}\\t{}\\t{}\\t{}\\n\".format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse),'\\t'.join(rrow)))\n",
    "\n",
    "for n in NN():\n",
    "    if F.otype.v(n) == 'book':\n",
    "        msg(\"\\t{}\".format(F.book.v(n)))\n",
    "    elif F.otype.v(n) == 'verse':\n",
    "        do_verse_info(n)\n",
    "\n",
    "wordf.close()\n",
    "#wordrf.close()\n",
    "plainf.close()\n",
    "msg(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK    word           word_heb       : max size =      27 of    32\n",
      "OK    word           word_ktv       : max size =      16 of    32\n",
      "OK    word           word_vlex      : max size =      23 of    32\n",
      "OK    word           word_clex      : max size =      15 of    32\n",
      "OK    word           word_tran      : max size =      30 of    32\n",
      "OK    word           word_phono     : max size =      23 of    32\n",
      "OK    word           word_lex       : max size =      15 of    32\n",
      "OK    word           word_glex      : max size =      24 of    32\n",
      "OK    word           word_gloss     : max size =      27 of    32\n",
      "OK    word           word_lang      : max size =       3 of     8\n",
      "OK    word           word_pos       : max size =       4 of     8\n",
      "OK    word           word_pdp       : max size =       4 of     8\n",
      "OK    word           word_subpos    : max size =       4 of     8\n",
      "OK    word           word_tense     : max size =       4 of     8\n",
      "OK    word           word_stem      : max size =       4 of     8\n",
      "OK    word           word_gender    : max size =       7 of     8\n",
      "OK    word           word_gnumber   : max size =       7 of     8\n",
      "OK    word           word_person    : max size =       7 of     8\n",
      "OK    word           word_state     : max size =       2 of     8\n",
      "OK    word           word_nme       : max size =       6 of     8\n",
      "OK    word           word_pfm       : max size =       6 of     8\n",
      "OK    word           word_prs       : max size =       6 of     8\n",
      "OK    word           word_uvf       : max size =       6 of     8\n",
      "OK    word           word_vbe       : max size =       3 of     8\n",
      "OK    word           word_vbs       : max size =       6 of     8\n",
      "OK    word           subphrase_border: max size =       8 of    16\n",
      "OK    word           subphrase_number: max size =      21 of    32\n",
      "OK    word           subphrase_rela : max size =       3 of     8\n",
      "OK    word           phrase_border  : max size =       5 of     8\n",
      "OK    word           phrase_atom_rela: max size =       4 of     8\n",
      "OK    word           phrase_function: max size =       4 of     8\n",
      "OK    word           phrase_rela    : max size =       4 of     8\n",
      "OK    word           phrase_typ     : max size =       4 of     8\n",
      "OK    word           phrase_det     : max size =       3 of     8\n",
      "OK    word           clause_border  : max size =       5 of     8\n",
      "OK    word           clause_atom_code: max size =       3 of     8\n",
      "OK    word           clause_rela    : max size =       4 of     8\n",
      "OK    word           clause_typ     : max size =       4 of     8\n",
      "OK    word           clause_txt     : max size =       7 of     8\n",
      "OK    word           sentence_border: max size =       5 of     8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR word           word_phono_sep : max size =      12 of     8\n"
     ]
    }
   ],
   "source": [
    "# check whether the field sizes are not exceeded\n",
    "\n",
    "tb = 'word'\n",
    "for f in word_fields:\n",
    "    (fl, typ, limit) = ('{}_{}'.format(f[2], f[1]), f[3], f[4])\n",
    "    if typ != 'varchar': continue\n",
    "    actual = field_sizes[tb][fl]\n",
    "    exceeded = actual > limit\n",
    "    outp = sys.stderr if exceeded else sys.stdout\n",
    "    outp.write('{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}\\n'.format(\n",
    "        'ERROR' if exceeded else 'OK',\n",
    "        tb, fl, actual, limit,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit_row = 2000\n",
    "\n",
    "tables_head = collections.OrderedDict((\n",
    "    ('book', 'insert into book (id, first_m, last_m, name) values \\n'),\n",
    "    ('chapter', 'insert into chapter (id, first_m, last_m, book_id, chapter_num) values \\n'),\n",
    "    ('verse', 'insert into verse (id, first_m, last_m, chapter_id, verse_num, text, xml) values \\n'),\n",
    "    ('clause_atom', 'insert into clause_atom (id, first_m, last_m, ca_num, book_id, text) values \\n'),\n",
    "    ('lexicon', 'insert into lexicon ({}) values \\n'.format(', '.join(f[2] for f in lex_fields))),\n",
    "    ('word', 'insert into word ({}) values \\n'.format(', '.join('{}_{}'.format(f[2], f[1]) for f in word_fields))),\n",
    "    ('word_verse', 'insert into word_verse (anchor, verse_id, lexicon_id) values \\n'),\n",
    "))\n",
    "\n",
    "sqf = outfile('shebanq_passage{}.sql'.format(version))\n",
    "sqf.write(text_create_sql)\n",
    "\n",
    "msg('Generating SQL ...')\n",
    "for table in tables_head:\n",
    "    msg('\\ttable {}'.format(table))\n",
    "    start = tables_head[table]\n",
    "    rows = tables[table]\n",
    "    r = 0\n",
    "    while r < len(rows):\n",
    "        sqf.write(start)\n",
    "        s = min(r + limit_row, len(rows))\n",
    "        sqf.write(' {}'.format(rows[r]))\n",
    "        if r + 1 < len(rows):\n",
    "            for t in rows[r + 1:s]: sqf.write('\\n,{}'.format(t))\n",
    "        sqf.write(';\\n')\n",
    "        r = s\n",
    "        \n",
    "sqf.close()\n",
    "msg('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
